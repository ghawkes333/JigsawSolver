{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "Yzxi1YwvKK5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "BMapp1dWKKXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d3b01b-a6df-4b81-9101-9bda3a9154c5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7Ot1x70Djr"
      },
      "source": [
        "# Generate permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37yJnYM0Djt",
        "outputId": "eb787f0b-60bb-4b0a-d490-f617e0307f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already performed count of iterations with pairs of jigsaw permutations 100\n",
            "Length of set of taken:  70\n",
            "No of iterations it took to build top - 100 permutations array = 141\n",
            "No of permutations 100\n",
            "Sample permutation 0\n",
            "[5 4 7 3 6 0 2 1 8]\n",
            "Sample permutation 1\n",
            "[6 8 3 1 5 0 7 2 4]\n",
            "Sample permutation 2\n",
            "[6 4 3 5 8 2 0 7 1]\n",
            "Sample permutation 3\n",
            "[0 1 2 7 4 6 5 8 3]\n",
            "Sample permutation 4\n",
            "[7 3 0 2 1 5 6 8 4]\n",
            "Sample permutation 5\n",
            "[1 0 7 8 3 6 2 4 5]\n",
            "Sample permutation 6\n",
            "[3 7 1 8 5 6 0 4 2]\n",
            "Sample permutation 7\n",
            "[8 1 4 6 2 5 7 3 0]\n",
            "Sample permutation 8\n",
            "[2 5 3 1 7 6 0 4 8]\n",
            "Sample permutation 9\n",
            "[7 3 0 2 4 6 5 8 1]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "\n",
        "# Build list of all possible permutations\n",
        "permuts_list = list(itertools.permutations(range(9)))\n",
        "permuts_array = np.array(permuts_list)\n",
        "no_permuts = len(permuts_list)\n",
        "\n",
        "\n",
        "# Take top x permutations which have max average hamming distance\n",
        "permuts_to_take = 100#200\n",
        "set_of_taken = set()\n",
        "cnt_iterations = 0\n",
        "while True:\n",
        "    cnt_iterations += 1\n",
        "    x = random.randint(1, no_permuts - 1)\n",
        "    y = random.randint(1, no_permuts - 1)\n",
        "    permut_1 = permuts_array[x]\n",
        "    permut_2 = permuts_array[y]\n",
        "    hd = hamming(permut_1, permut_2)\n",
        "\n",
        "    if hd > 0.9 and (not x in set_of_taken) and (not y in set_of_taken):\n",
        "        set_of_taken.add(x)\n",
        "        set_of_taken.add(y)\n",
        "\n",
        "        if len(set_of_taken) == permuts_to_take:\n",
        "            break\n",
        "\n",
        "    if cnt_iterations % 100 == 0:\n",
        "        print (\"Already performed count of iterations with pairs of jigsaw permutations\", cnt_iterations)\n",
        "        print (\"Length of set of taken: \",len(set_of_taken))\n",
        "\n",
        "print (\"No of iterations it took to build top - {} permutations array = {}\".format(permuts_to_take, cnt_iterations))\n",
        "print (\"No of permutations\", len(set_of_taken))\n",
        "\n",
        "\n",
        "# Build the array for selected permutation indices above\n",
        "selected_permuts = []\n",
        "for ind, perm_id in enumerate(set_of_taken):\n",
        "    if ind < 10:\n",
        "        print (\"Sample permutation {}\".format(ind))\n",
        "        print (permuts_array[perm_id])\n",
        "    selected_permuts.append(permuts_array[perm_id])\n",
        "\n",
        "selected_permuts = np.array(selected_permuts)\n",
        "np.save('selected_permuts.npy', selected_permuts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "N5DPoz-a0Djv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "def_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "hflip_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "darkness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 0.9]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lightness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[1.1, 1.5]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "rotations_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "all_in_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def crop_from_center(pil_image, new_h, new_w):\n",
        "\n",
        "    width, height = pil_image.size  # Get dimensions\n",
        "\n",
        "    left = (width - new_w) / 2\n",
        "    top = (height - new_h) / 2\n",
        "    right = (width + new_w) / 2\n",
        "    bottom = (height + new_h) / 2\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def get_nine_crops(pil_image):\n",
        "    \"\"\"\n",
        "    Get nine crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/3)\n",
        "\n",
        "    r_vals = [0, diff, 2 * diff]\n",
        "    c_vals = [0, diff, 2 * diff]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def split_train_into_train_val(train_file_ids, train_file_paths, train_labels, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Split train_file_paths and train_labels to train_file_paths, val_file_paths and\n",
        "    train_labels, val_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mapping between image_id and file_path\n",
        "    image_id_name_map = dict(zip(train_file_ids, train_file_paths))\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_file_ids, val_file_ids, train_labels, val_labels = train_test_split(\n",
        "        train_file_ids, train_labels, test_size=test_size, random_state=5, shuffle=True\n",
        "    )\n",
        "    train_file_paths = [image_id_name_map[image_id] for image_id in train_file_ids]\n",
        "    val_file_paths = [image_id_name_map[image_id] for image_id in val_file_ids]\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels\", len(train_labels))\n",
        "    print (\"Length of val files list\", len(val_file_paths))\n",
        "    print (\"Length of val labels\", len(val_labels))\n",
        "\n",
        "    return train_file_ids, val_file_ids, train_file_paths, val_file_paths, train_labels, val_labels\n",
        "\n",
        "def get_paths(data_dir):\n",
        "    file_paths_to_return = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_paths_to_return.append(data_dir+'/'+file)\n",
        "\n",
        "    return file_paths_to_return\n",
        "\n",
        "def get_train_test_file_paths_n_labels():\n",
        "    \"\"\"\n",
        "    Get array train_file_paths, train_labels, test_file_paths and test_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    #par_data_dir = 'train'\n",
        "    images_data_dir = 'train'\n",
        "    train_test_split_file = 'train_test_split.txt'\n",
        "    images_file = 'images.txt'\n",
        "    labels_file = 'image_class_labels.txt'\n",
        "\n",
        "    # Read the images_file which stores image-id and image-name mapping\n",
        "    image_file_id_df = pd.read_csv(images_file, sep=' ', header=None)\n",
        "    image_file_id_mat = image_file_id_df.values\n",
        "    image_id_name_map = dict(zip(image_file_id_mat[:, 0], image_file_id_mat[:, 1]))\n",
        "\n",
        "    # Read the train_test_split file which stores image-id and train-test split mapping\n",
        "    image_id_train_test_split_df = pd.read_csv(train_test_split_file, sep=' ', header=None)\n",
        "    image_id_train_test_split_mat = image_id_train_test_split_df.values\n",
        "    image_id_train_test_split_map = dict(zip(image_id_train_test_split_mat[:, 0],\n",
        "                                             image_id_train_test_split_mat[:, 1]))\n",
        "\n",
        "    # Read the image class labels file\n",
        "    image_id_label_df = pd.read_csv(labels_file, sep=' ', header=None)\n",
        "    image_id_label_mat = image_id_label_df.values\n",
        "    image_id_label_map = dict(zip(image_id_label_mat[:, 0], image_id_label_mat[:, 1]))\n",
        "\n",
        "    # Put together train_files train_labels test_files and test_labels lists\n",
        "    train_image_ids, test_image_ids = [], []\n",
        "    train_file_paths, test_file_paths = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "    for file_id in image_id_name_map.keys():\n",
        "        file_name = image_id_name_map[file_id]\n",
        "        is_train = image_id_train_test_split_map[file_id]\n",
        "        label = image_id_label_map[file_id] - 1  # To ensure labels start from 0\n",
        "\n",
        "        if is_train:\n",
        "            train_image_ids.append(file_id)\n",
        "            train_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            train_labels.append(label)\n",
        "        else:\n",
        "            test_image_ids.append(file_id)\n",
        "            test_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels list\", len(train_labels))\n",
        "    print (\"Length of test files list\", len(test_file_paths))\n",
        "    print (\"Length of test labels list\", len(test_labels))\n",
        "\n",
        "    return train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UIXFNq0Djw"
      },
      "source": [
        "# Generate Jigsaw from permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8SFOvvMU0Djw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#from dataset_helpers import crop_from_center, get_nine_crops\n",
        "\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        'Initialization'\n",
        "        self.imgs = [(img_path, label) for img_path, label in zip(file_paths, labels)]\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            label = self.labels[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        tr_image = self.transform(pil_image)\n",
        "\n",
        "        return tr_image, label\n",
        "\n",
        "\n",
        "class GetJigsawPuzzleDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None):\n",
        "        'Initialization'\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
        "        self.range_permut_indices = range_permut_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        pil_image = pil_image.resize((256, 256))\n",
        "        pil_image = crop_from_center(pil_image, 225, 225)\n",
        "\n",
        "        # Get nine crops for the image\n",
        "        nine_crops = get_nine_crops(pil_image)\n",
        "\n",
        "        # Permut the 9 patches obtained from the image\n",
        "        if self.range_permut_indices:\n",
        "            permut_ind = random.randint(self.range_permut_indices[0], self.range_permut_indices[1])\n",
        "        else:\n",
        "            permut_ind = random.randint(0, len(self.permuts_avail) - 1)\n",
        "\n",
        "        permutation_config = self.permuts_avail[permut_ind]\n",
        "\n",
        "        permuted_patches_arr = [None] * 9\n",
        "        for crop_new_pos, crop in zip(permutation_config, nine_crops):\n",
        "            permuted_patches_arr[crop_new_pos] = crop\n",
        "\n",
        "        # Apply data transforms\n",
        "        tensor_patches = torch.zeros(9, 3, 64, 64)\n",
        "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
        "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
        "            tensor_patches[ind] = jigsaw_patch_tr\n",
        "\n",
        "        return tensor_patches, permut_ind\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R30hrWv0Djx"
      },
      "source": [
        "# Defining Resnet model\n",
        "Credit: https://github.com/aniket03/self_supervised_bird_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gMNaA-Q80Djx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None, siamese_deg=9, train_contrastive=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.siamese_deg = siamese_deg\n",
        "        self.train_contrastive = train_contrastive\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        if self.siamese_deg is None:\n",
        "            self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
        "        else:\n",
        "            self.fc = nn.Linear(2048 * block.expansion * self.siamese_deg, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_feature_vectors(self, input_batch):\n",
        "        # Each input_batch would be of shape (batch_size, color_channels, h, w)\n",
        "        x = self.conv1(input_batch)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "\n",
        "        # Data returned by data loaders is of the shape (batch_size, no_patches, h_patch, w_patch)\n",
        "        # That's why named input to patches_batch\n",
        "\n",
        "        if self.siamese_deg is None:\n",
        "            final_feat_vectors = self.get_feature_vectors(input_batch)\n",
        "            x = F.dropout(final_feat_vectors)\n",
        "            x = F.log_softmax(self.fc(x))\n",
        "        elif not self.train_contrastive:\n",
        "            final_feat_vectors = None\n",
        "            for patch_ind in range(self.siamese_deg):\n",
        "                # Each patch_batch would be of shape (batch_size, color_channels, h_patch, w_patch)\n",
        "                patch_batch = input_batch[:, patch_ind, :, :, :]\n",
        "                patch_batch_features = self.get_feature_vectors(patch_batch)\n",
        "\n",
        "                if patch_ind == 0:\n",
        "                    final_feat_vectors = patch_batch_features\n",
        "                else:\n",
        "                    final_feat_vectors = torch.cat([final_feat_vectors, patch_batch_features], dim=1)\n",
        "            x = F.dropout(final_feat_vectors)\n",
        "            x = F.log_softmax(self.fc(x), dim=0)# Shape is [4, 100]\n",
        "        else:\n",
        "            q_img_batch = input_batch[:, 0, :, :, :]\n",
        "            p_img_batch = input_batch[:, 1, :, :, :]\n",
        "            n_img_batch = input_batch[:, 2, :, :, :]\n",
        "\n",
        "            q_img_batch_feats = self.get_feature_vectors(q_img_batch)\n",
        "            p_img_batch_feats = self.get_feature_vectors(p_img_batch)\n",
        "            n_img_batch_feats = self.get_feature_vectors(n_img_batch)\n",
        "\n",
        "            pos_sq_dist = torch.norm(q_img_batch_feats - p_img_batch_feats, p=2, dim=1) ** 2\n",
        "            neg_sq_dist = torch.norm(q_img_batch_feats - n_img_batch_feats, p=2, dim=1) ** 2\n",
        "\n",
        "            x = pos_sq_dist - neg_sq_dist\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    \"\"\"\n",
        "    return _resnet(BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hdr6VxdF0Djy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "def get_count_correct_preds(network_output, target):\n",
        "\n",
        "    output = network_output\n",
        "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "    pred.data = pred.data.view_as(target.data)\n",
        "    correct = target.eq(pred).sum().item()\n",
        "\n",
        "    return correct\n",
        "\n",
        "\n",
        "class ModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(ModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "              data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "              output = self.network(data)\n",
        "              test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "              correct += get_count_correct_preds(output, target)\n",
        "\n",
        "              del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n",
        "\n",
        "\n",
        "class JigsawModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(JigsawModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "              data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "              output = self.network(data)\n",
        "              test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "              correct += get_count_correct_preds(output, target)\n",
        "\n",
        "              del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlmHNV6o0Djy"
      },
      "source": [
        "# Jigsaw as pretext task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "noZzi5Bw0Djy",
        "outputId": "4ce31f69-67ad-4d41-8474-c2394a5a4a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 9, 3, 64, 64])\n",
            "torch.Size([4])\n",
            "Model ready\n",
            "Started training\n",
            "Epoch no 0 #######################\n",
            "Not train contras\n",
            "tensor([[-8.0447e-01,  5.1190e-01, -8.8800e-01,  1.2423e+00, -4.6460e-02,\n",
            "          7.5208e-01,  1.9465e+00,  1.1144e+00,  8.3612e-01,  8.2302e-01,\n",
            "         -1.8826e+00, -1.1631e-01, -2.2563e-01,  4.8346e-01,  4.7854e-01,\n",
            "          1.7059e+00,  1.3324e+00,  9.9081e-01, -1.4986e+00,  1.3256e+00,\n",
            "         -3.6463e-01, -8.9625e-01, -1.7689e+00,  9.8655e-01,  1.1075e+00,\n",
            "         -8.6511e-02,  1.2957e+00,  3.4775e-02,  3.3457e-01, -1.0813e+00,\n",
            "         -6.8015e-01, -2.1612e-01, -7.8599e-01,  2.0672e+00, -2.7728e-01,\n",
            "          1.9285e-01, -2.0623e-02, -6.1461e-01,  1.4431e+00,  5.7496e-02,\n",
            "          1.2425e+00,  5.1699e-01,  3.3311e-01, -1.4022e+00,  1.1218e-01,\n",
            "          2.3963e-01, -4.7845e-01, -1.1789e+00,  5.1267e-01,  5.9785e-01,\n",
            "          1.1314e+00,  8.7753e-01,  3.6923e-01,  1.8583e+00,  1.4540e+00,\n",
            "         -8.8871e-01,  2.1272e+00, -3.2363e-01,  7.0464e-02, -5.7405e-01,\n",
            "          3.5446e-01, -8.7613e-01,  1.3802e-01, -1.4206e+00, -1.6941e+00,\n",
            "          4.6831e-01,  1.4177e+00, -5.0383e-01,  1.3212e+00,  1.0449e+00,\n",
            "          1.6515e+00, -1.1845e-02, -7.0087e-01, -1.7892e-01, -9.6522e-01,\n",
            "          4.3040e-01,  2.2774e-01, -2.5983e-01, -3.1205e-01, -6.2388e-02,\n",
            "         -1.2481e+00,  3.3268e-01, -3.5252e-01,  1.3998e+00, -9.7849e-01,\n",
            "          1.4374e+00, -8.2365e-01, -4.1362e-03, -2.1846e+00,  1.3143e+00,\n",
            "          6.8780e-01, -2.9498e-01,  4.8184e-01, -2.1509e+00,  3.0053e-01,\n",
            "          5.0255e-01,  4.3367e-01,  3.5904e-01,  1.8452e+00, -5.7506e-01],\n",
            "        [-3.6860e-01,  9.5992e-02, -9.5688e-02,  1.9488e-01,  3.4557e+00,\n",
            "          1.8489e-01,  1.0801e+00, -7.7214e-01,  2.4738e-01,  7.9192e-01,\n",
            "          3.6618e-01,  3.5521e-02, -1.8327e+00, -7.1691e-01, -5.3876e-01,\n",
            "          2.2693e+00,  4.7459e-01,  1.4804e+00, -2.2827e-01,  4.0802e-01,\n",
            "         -1.1749e-01, -1.4900e+00,  1.2577e+00,  1.3329e+00, -4.7802e-02,\n",
            "         -2.3597e-01,  1.9485e+00, -1.3692e+00, -5.2508e-01, -3.1175e+00,\n",
            "         -1.3026e+00, -1.8701e-01,  1.2952e+00,  2.3100e-01,  1.2424e+00,\n",
            "          4.7901e-02, -1.0948e+00,  1.0697e-01, -1.2889e+00,  1.6487e+00,\n",
            "          1.0420e+00, -3.9914e-01,  3.0513e-01, -1.3724e-02, -7.3563e-01,\n",
            "          1.3515e-01,  2.0964e+00,  6.8638e-01,  1.1565e+00,  7.9129e-01,\n",
            "         -4.5306e-02,  1.4466e+00, -1.2582e+00,  2.0399e+00, -8.8665e-01,\n",
            "          6.0170e-02, -3.4846e-01, -1.1019e+00,  1.1194e+00, -1.9080e+00,\n",
            "          5.9139e-01, -1.0796e+00,  3.3344e-01,  8.6508e-01,  2.0689e-01,\n",
            "          4.1565e-01,  7.2644e-01, -2.1082e+00,  2.2527e-01,  9.2667e-02,\n",
            "          3.5967e+00, -1.3814e+00, -1.8734e-02,  9.2046e-01, -8.3514e-01,\n",
            "         -1.1018e+00,  8.8667e-01, -8.9767e-01,  5.8964e-01, -2.7342e-01,\n",
            "          1.0735e+00, -4.1041e-01, -1.3146e+00,  7.3729e-01, -2.5220e+00,\n",
            "          3.6436e-01, -6.3448e-01,  1.5743e-01,  1.0010e+00,  7.2105e-02,\n",
            "         -5.7240e-01,  1.3386e-01, -1.6362e+00, -2.3598e+00, -1.5370e+00,\n",
            "          1.6487e-01,  8.1800e-01,  1.5132e+00,  9.7636e-01, -7.7682e-01],\n",
            "        [-9.5612e-01, -2.7182e-01, -4.0058e-01, -1.0595e+00,  1.3768e+00,\n",
            "          3.5120e-01, -1.0252e-01,  2.9956e-01,  4.3470e-01,  8.1251e-01,\n",
            "          1.2724e+00,  7.0074e-01, -6.3144e-01,  1.5094e+00,  4.1863e-01,\n",
            "          1.1600e+00,  2.4217e-01, -2.4679e+00,  9.5578e-01, -7.1998e-01,\n",
            "          1.2722e+00, -8.6649e-01, -1.9594e-01,  1.5750e+00, -1.5211e+00,\n",
            "         -1.3265e+00, -6.4052e-01,  3.3968e-02, -4.1461e-01,  9.4343e-01,\n",
            "         -9.8675e-01, -4.6429e-01, -1.3583e+00, -2.5772e-01, -1.9346e-01,\n",
            "         -4.7124e-01, -9.7647e-01, -1.8812e+00, -7.9580e-01,  3.6685e-01,\n",
            "          3.1866e-01,  1.8734e-01,  8.1619e-01, -1.7055e+00, -1.7883e-01,\n",
            "         -3.4203e-01,  2.0022e+00, -1.3864e+00,  1.5996e+00, -7.7632e-02,\n",
            "          1.0218e+00,  9.1151e-02, -5.3738e-01,  1.4023e+00, -1.6727e-01,\n",
            "          1.1154e+00,  1.1869e+00, -4.2385e-01,  1.4207e+00,  2.8474e-01,\n",
            "          8.4789e-01, -5.1067e-01, -5.1755e-01, -5.0216e-01,  7.4438e-01,\n",
            "          6.3243e-01,  3.2508e-01,  2.1911e-01,  9.3846e-01, -1.9759e-01,\n",
            "          1.2958e+00,  1.0197e+00,  7.2156e-01,  2.2596e-01, -1.6830e-01,\n",
            "         -1.4275e-02, -2.6956e-02,  8.4622e-01, -9.4700e-01,  4.2743e-01,\n",
            "         -5.7434e-01, -4.2758e-01, -1.0113e-01, -4.8022e-03, -1.5205e+00,\n",
            "         -3.3041e-01, -1.7439e+00, -8.4008e-01,  3.1241e-01,  2.1297e+00,\n",
            "         -7.2187e-01,  1.0184e+00, -1.5777e+00,  6.3151e-01, -4.0645e-02,\n",
            "         -1.5162e+00, -5.6034e-01,  8.9781e-01,  1.7938e+00,  1.6566e+00],\n",
            "        [ 1.0148e+00, -2.0774e-02,  5.3625e-01,  7.9803e-01, -2.5385e-01,\n",
            "          2.2366e-01, -1.4479e+00, -1.2488e+00, -4.0913e-01,  1.1299e+00,\n",
            "          6.4844e-01,  9.6335e-02, -1.5749e-01, -1.0358e+00, -6.8925e-04,\n",
            "          2.6474e-01,  1.9489e+00,  5.1508e-01,  7.0280e-01,  1.4477e+00,\n",
            "          1.5113e+00, -4.6355e-01, -4.9613e-01, -1.1201e+00, -5.3470e-01,\n",
            "          5.6911e-01,  1.5638e+00, -1.6639e+00, -1.3985e+00,  1.2095e-01,\n",
            "         -1.3693e-01,  1.2307e+00,  9.7930e-01, -9.2713e-01, -4.2531e-01,\n",
            "         -2.3630e-01, -3.2617e-01,  1.1344e-01, -1.4619e+00, -8.0933e-01,\n",
            "          7.5719e-01,  1.3099e+00, -9.9908e-01, -7.6995e-01,  1.2501e+00,\n",
            "         -1.1448e+00, -2.5371e-01, -3.8430e-01, -5.0261e-01, -5.6911e-02,\n",
            "          6.4714e-01, -4.1026e-02, -2.1426e-01, -1.0520e+00, -1.2958e-01,\n",
            "         -1.5882e+00,  8.1646e-01, -1.0693e+00,  6.9233e-01, -6.1640e-02,\n",
            "          9.9911e-01, -1.6667e+00,  1.7881e+00,  1.8262e+00,  7.6750e-01,\n",
            "         -3.9419e-02,  3.2255e-01, -3.1002e-01, -1.0260e-01,  1.5430e+00,\n",
            "          1.4802e+00,  7.0114e-01,  2.7812e-01,  4.1925e-03,  1.1323e-01,\n",
            "         -8.5530e-01,  1.3475e+00,  7.8921e-01, -1.3570e+00,  5.7641e-01,\n",
            "          2.9981e-01, -9.5964e-01, -9.5022e-01,  1.0913e+00, -3.2905e-02,\n",
            "         -5.6713e-01,  7.8087e-01, -7.8389e-01, -7.6658e-01,  6.7126e-01,\n",
            "         -9.7613e-03, -6.9026e-01,  6.8625e-01, -9.7417e-01, -6.6725e-01,\n",
            "          9.5378e-01, -6.5181e-01,  8.8839e-01,  2.0813e+00,  2.0152e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "torch.Size([4, 100])\n",
            "Not train contras\n",
            "tensor([[ 0.6497,  2.1172, -0.7983, -0.8719,  0.7324,  0.9009, -0.4413, -0.5256,\n",
            "          0.3977,  0.2850, -0.9219, -0.3298, -0.0655,  1.0919, -0.3292,  1.7152,\n",
            "          3.0446,  0.7837, -1.3034, -0.7210,  2.3271, -0.0137, -0.2569,  1.4363,\n",
            "         -0.8525,  0.7809, -2.6656, -2.5356, -0.9923, -1.1612, -0.4976,  1.4403,\n",
            "          0.3795,  0.0974, -0.3007, -0.1441,  1.8026, -0.7350, -2.4622, -0.5318,\n",
            "          0.6087,  0.4757,  1.9712, -0.6456,  0.7665, -0.7621, -2.7942, -0.9221,\n",
            "         -0.0101, -0.0494, -0.2032,  0.9793, -0.3730,  0.3000,  1.8105,  0.3807,\n",
            "          0.2154, -3.8952,  1.0869, -0.2831,  1.3895, -1.0771,  0.6346,  0.6518,\n",
            "         -1.1280,  0.4280,  0.9235, -2.3042, -0.3601,  1.6485,  0.0789,  0.7042,\n",
            "         -0.6068,  0.3770, -1.3669, -0.0933, -0.2920,  0.6770, -0.0559, -0.0322,\n",
            "         -0.5464, -0.1613, -1.1648, -0.5539, -3.2592, -0.5045, -0.6263, -0.4855,\n",
            "          0.4227,  1.2192,  0.4424, -0.6322, -1.4733,  0.9302, -0.6499, -0.0368,\n",
            "         -0.0054, -0.9642,  2.0368, -1.1698],\n",
            "        [ 0.2749,  1.3782, -0.8717,  1.7959,  0.3757, -0.3766,  0.4918, -0.7037,\n",
            "         -0.0402,  0.4252,  2.2237, -0.3495, -1.4809,  1.4726, -0.3135,  0.4311,\n",
            "          0.3849,  0.1350,  1.1329,  0.9000,  1.5407, -0.1392,  0.3531, -0.1295,\n",
            "          0.0682,  0.0237, -0.0610, -2.4381,  0.0137, -0.3508, -0.9940,  0.8444,\n",
            "         -0.8251,  0.6227, -1.5278, -0.8669, -2.0896,  0.1109, -1.1369, -0.3078,\n",
            "          0.3779, -0.2889, -0.0181,  0.6138,  0.8516,  1.1732,  2.4225, -0.0230,\n",
            "         -0.5577, -0.4731,  1.0517,  1.0064, -2.0745,  1.8784, -1.2392, -0.0244,\n",
            "         -0.4897, -1.6883, -0.3808, -1.2788, -0.6244, -0.9768,  0.1282, -0.8414,\n",
            "          1.5475,  0.3386,  2.1173, -1.0760,  0.3463,  0.1525,  1.7426,  1.7669,\n",
            "         -0.2782, -0.2330, -0.5214, -0.9209,  0.9844, -1.9189, -1.2775,  0.3104,\n",
            "         -1.9747, -0.0377,  0.7517,  0.0996,  0.8784,  2.0950,  0.8611,  0.5894,\n",
            "          0.7580,  1.4563,  0.2992,  1.2765, -0.8466,  0.2887, -0.9497, -0.7280,\n",
            "         -1.4930, -0.1273,  1.6836,  0.7528],\n",
            "        [ 0.0131,  0.2163, -0.3880, -0.2344, -0.6823,  0.0912,  0.2653, -0.7326,\n",
            "         -1.0559,  0.6539, -0.2350, -0.6587,  0.3504, -1.2804, -1.3614, -1.9296,\n",
            "          0.3067, -0.5785, -1.3206,  0.3900, -1.5702, -1.6529, -0.0678,  1.4269,\n",
            "         -1.0213,  0.1368,  0.4399, -0.9699,  0.3160, -0.0968, -1.8381,  0.6971,\n",
            "          0.9631,  0.9055, -0.3326, -0.3292, -1.4655, -0.6307, -2.2460,  0.8055,\n",
            "          1.1273,  0.3233,  0.6043, -1.0678,  0.1018, -0.3487, -0.9225,  1.2121,\n",
            "          1.5574,  0.0732,  1.0459, -0.6855, -2.1248, -0.0693, -0.1543, -0.5160,\n",
            "         -0.9756, -1.9095, -0.0152,  0.9698,  1.7594, -1.6638,  1.8288,  2.1402,\n",
            "          0.0674,  0.7008, -0.3869, -0.6827, -0.5986, -2.4358,  1.5095,  0.8318,\n",
            "          0.6821,  0.5898, -0.2985,  0.5234,  1.0442,  0.4177, -2.3548, -0.1061,\n",
            "         -0.8291, -0.5629, -0.3725,  0.4813, -0.4843,  0.4448, -0.5823, -1.2684,\n",
            "         -0.2814,  0.2975,  0.3618,  0.3157, -1.1454, -1.9712,  0.2207,  0.1064,\n",
            "         -1.2597,  0.7110,  0.0317,  0.1831],\n",
            "        [-0.2333, -0.3112, -1.5758,  0.4399,  1.0177,  0.8865,  0.2009,  0.0937,\n",
            "          1.4765, -0.4895,  0.4508,  0.1184, -0.4012,  0.8530, -0.7295, -0.1818,\n",
            "          1.5655,  0.3876, -2.3928,  0.8609,  0.5866, -1.5274,  0.6398, -1.3583,\n",
            "          0.2713, -0.3400, -0.2877, -0.9653, -1.0917,  0.7516,  0.7528,  0.6742,\n",
            "          0.2040, -0.2441, -0.1108, -2.0208, -2.3114, -0.6252, -0.5353, -0.0366,\n",
            "          0.5138,  1.2020,  1.8898, -0.1339, -0.3416,  0.1635,  0.0583, -0.4984,\n",
            "          1.2105,  0.7109, -0.2142, -1.3095, -0.2623,  0.4209,  0.5799, -0.0531,\n",
            "          0.9502, -0.9922, -0.4098, -0.7934,  0.4183,  0.3947, -1.4657,  1.0476,\n",
            "         -0.9104,  0.3239, -0.5085, -0.3456, -0.0229, -0.4725, -1.2061,  0.3567,\n",
            "         -0.9665, -1.1300, -0.9787,  0.1351,  1.4359,  0.6216, -0.0616, -0.5804,\n",
            "          1.1441,  1.9610, -0.1160,  1.9157, -1.3307, -0.4072,  0.2141,  1.5285,\n",
            "         -1.3502,  3.8114, -0.8274, -0.4477, -1.5356, -0.9508,  0.2243, -1.6865,\n",
            "          0.6190,  0.9123,  0.1968,  0.4279]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([4, 100])\n",
            "Not train contras\n",
            "tensor([[ 0.5485, -0.7378,  0.2206,  0.3981,  0.9086,  0.5478,  2.0759, -1.1350,\n",
            "         -0.1653,  1.0130,  1.2926, -0.0617, -0.0215,  0.4553,  0.7324, -0.1429,\n",
            "         -0.0932,  0.5391, -0.7502,  1.9342,  0.2919, -0.2680, -0.7287,  0.5214,\n",
            "         -0.4524,  0.8408, -1.1870, -0.5011, -0.3919, -0.0686, -1.4602, -0.6912,\n",
            "         -0.3555,  0.9600, -1.3996, -1.1592, -2.4881, -0.3818, -0.2058,  0.7900,\n",
            "          0.5057,  0.9311,  0.9980,  0.2634, -0.2825,  0.7444, -1.7510,  0.5931,\n",
            "          0.2932,  1.9893,  0.8051,  1.1429, -1.6547,  0.2259,  0.5468, -1.4117,\n",
            "          0.1619, -1.3162,  0.6543, -1.7236,  0.7874,  0.0288, -0.2280,  1.0891,\n",
            "         -0.2441,  0.3719,  0.5948, -2.1023, -1.1138,  0.2319, -1.0723, -0.5667,\n",
            "         -1.2786,  0.3816,  0.4155, -1.2268,  1.7835, -0.7224, -1.6688,  0.4339,\n",
            "         -1.0506,  1.0925, -0.4032, -0.2189, -1.0753,  2.2775, -0.2842, -0.8498,\n",
            "         -0.5530,  1.1635, -1.2521, -1.4861, -0.2473, -0.9994, -2.3186,  0.4484,\n",
            "          0.2533, -0.5377,  0.3451, -0.2485],\n",
            "        [ 0.9196,  0.4451,  0.0274, -0.4130,  0.0237, -0.1766, -0.4922,  1.6574,\n",
            "          0.2705,  0.0941, -1.2294, -0.3640,  0.9634,  0.3726, -1.2039,  0.5429,\n",
            "          1.7770,  0.5783, -1.2364,  1.1276,  0.2427, -0.5327,  0.8804,  0.7843,\n",
            "         -2.4345, -1.4586, -0.3748, -0.1191, -0.0404, -2.1881,  0.0391,  0.0508,\n",
            "          0.4166, -0.6613, -1.0769, -1.0792, -1.5232,  0.8843, -1.8126,  0.6050,\n",
            "          0.1749,  0.2486,  1.1357,  0.5600, -0.9837, -0.7278,  1.1742, -0.7290,\n",
            "          0.2474, -0.0811, -0.1106, -0.0284, -1.0946,  0.2038, -1.0599,  0.8374,\n",
            "          0.2777, -1.6099,  0.1409, -0.2563,  0.7091, -1.6081,  1.1646, -1.9238,\n",
            "         -1.5521,  0.4567, -0.2896, -1.7244, -1.6376,  1.7987,  2.1783,  0.8288,\n",
            "         -1.1333,  0.4397,  0.7174, -0.4286, -0.7172, -0.8754, -0.2966, -0.0717,\n",
            "         -1.5267,  1.5614, -1.0171,  1.6518, -0.6120, -0.1388, -0.7347, -1.1411,\n",
            "         -0.3021,  1.5308,  0.8808,  0.3301, -1.2820, -1.4512,  0.7957,  1.1670,\n",
            "         -0.5144,  1.6911,  1.9180, -0.7353],\n",
            "        [-0.7981, -0.4363, -1.7061,  1.4469,  2.4308,  1.0330,  2.1123, -0.6390,\n",
            "          0.3874,  0.0171, -1.9160, -1.4418, -0.0443, -0.0049, -1.4094,  0.3487,\n",
            "          0.7915, -0.5084, -1.2578,  1.1600,  0.8404, -0.4815,  1.7287,  0.0683,\n",
            "          0.1312,  0.5216,  0.4179, -0.0911, -1.1868,  0.1340, -1.0612,  1.5388,\n",
            "         -0.2622, -0.1624, -0.4901, -0.6541, -0.1142, -0.7048, -1.0978, -0.7350,\n",
            "          1.0376, -1.8978, -0.5351,  1.6313,  0.1268, -1.8631,  0.6806, -1.0646,\n",
            "          0.6354,  1.0199, -0.1017,  0.9751, -0.2743,  0.1586,  1.3135, -0.6046,\n",
            "          0.8773, -0.4772,  0.9399, -0.1512,  1.7303, -1.1780, -0.3004,  1.4865,\n",
            "          0.4255, -0.2947,  2.9379,  0.5021,  1.9320, -0.0215,  1.9394,  0.2389,\n",
            "          1.1084,  0.5376,  0.3272,  1.9446, -1.7385,  2.0232,  0.4768, -1.2261,\n",
            "         -1.3628, -0.0990,  0.2185, -0.5560, -0.4090,  0.1747,  0.2041, -0.0622,\n",
            "          0.2557,  0.8433, -0.3514, -0.8869, -0.1901, -1.6119, -0.6828, -0.0185,\n",
            "          0.7766,  1.5586,  1.5853,  0.5434],\n",
            "        [-0.5984,  0.0039, -2.3150,  0.3300, -0.2258,  0.3648,  0.3138,  0.3862,\n",
            "          0.3472, -0.0357, -0.8629,  0.4403,  1.0794,  0.9311,  0.2207,  1.9058,\n",
            "          2.0537,  1.0757, -0.3115, -0.1333,  0.4895, -1.3829, -0.6109,  0.6348,\n",
            "          0.3055,  0.5572, -0.4049, -0.9817,  1.6679, -0.0876, -0.8294,  0.3851,\n",
            "         -0.4480, -0.0236, -0.9919, -1.2217,  0.0559,  1.1591,  0.5507,  1.1804,\n",
            "          0.4751,  1.2146,  1.2892, -0.7361, -0.0896,  0.1069, -0.1545, -1.4747,\n",
            "         -0.7887,  0.7973,  0.3102,  1.0484, -0.1886, -2.4200, -0.8404, -0.4110,\n",
            "          2.5350, -1.0423,  0.5654, -1.1366,  1.4358, -0.2673,  1.4389, -1.5400,\n",
            "         -2.2696,  0.8004, -0.4285, -0.3662, -0.5109, -0.9731,  0.1258,  0.3106,\n",
            "         -1.8879, -0.1243, -1.6519,  1.5822,  1.1135,  1.0443, -0.3011, -0.6570,\n",
            "         -2.4914,  0.5147,  0.1244, -0.7105, -1.1697,  1.6082,  0.7783, -0.6872,\n",
            "         -0.6092,  3.0543, -0.8517, -0.5032, -1.2214,  0.7174,  0.0473,  0.5816,\n",
            "         -1.2077, -0.2144, -0.3152,  0.5626]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([4, 100])\n",
            "Not train contras\n",
            "tensor([[-7.4022e-01,  1.1777e+00, -6.3129e-01,  6.1754e-01, -4.4437e-01,\n",
            "          2.3957e-02,  8.7169e-01,  1.0460e+00,  7.6876e-02, -7.8661e-01,\n",
            "         -9.9785e-01,  2.5875e-01, -4.3319e-01,  1.1147e+00, -4.0387e-01,\n",
            "         -2.1352e-01,  1.3588e-01, -1.9585e+00, -1.1846e+00,  4.9107e-02,\n",
            "          8.2336e-01, -1.5443e+00,  6.3929e-01, -2.2744e-01, -2.0028e-01,\n",
            "         -8.9839e-02, -9.1190e-01, -8.0345e-01, -6.9255e-01,  6.4883e-01,\n",
            "          6.1883e-01,  2.9056e-01, -9.4882e-01,  2.6330e-01, -1.0874e+00,\n",
            "          1.7043e+00,  1.9910e-01,  1.6342e-01, -1.1826e+00, -1.0823e+00,\n",
            "         -9.6156e-02,  1.7677e-02, -4.1283e-01, -2.8771e+00,  9.7790e-01,\n",
            "         -6.7946e-01, -1.0401e+00, -4.8871e-01,  1.2371e+00,  9.7697e-01,\n",
            "         -6.4518e-01,  3.6359e-01, -2.0255e-01,  8.2389e-01, -2.5496e+00,\n",
            "         -1.4248e-01,  5.6277e-01, -8.0913e-01, -4.8466e-01,  3.6548e-01,\n",
            "          1.3038e+00,  2.7703e-01, -8.0639e-01,  5.3437e-01, -1.3995e+00,\n",
            "          1.7758e+00,  9.5870e-01, -3.7800e-01,  5.8843e-01,  5.4474e-01,\n",
            "          1.1230e+00,  2.8342e-01, -1.8408e-01, -5.4336e-01, -8.2617e-01,\n",
            "         -4.9161e-01,  5.4662e-01, -1.5227e-01, -1.8605e+00, -1.0956e+00,\n",
            "          1.1690e+00,  9.8583e-01,  5.9109e-01,  4.1897e-01, -7.5226e-01,\n",
            "         -7.6689e-01, -8.5164e-01, -1.5480e+00,  8.5789e-01,  2.0235e+00,\n",
            "          1.3971e+00,  7.6746e-01, -2.9429e+00, -5.7045e-01, -1.5587e+00,\n",
            "         -1.4068e+00,  1.0528e+00,  9.4175e-02,  8.6860e-01, -1.6376e-01],\n",
            "        [ 2.0061e-01,  1.7815e+00, -1.1648e+00, -2.6405e-01,  1.8355e+00,\n",
            "          1.0531e+00,  1.0280e+00,  1.2400e+00,  1.3696e+00, -1.4249e-01,\n",
            "          1.5135e+00,  1.5658e+00, -2.3929e+00,  1.1021e+00, -8.2502e-01,\n",
            "          9.6682e-01,  2.1956e+00, -1.3629e+00, -4.8757e-01,  6.1276e-01,\n",
            "         -7.0183e-01,  1.4566e+00,  1.8398e-01,  1.5372e+00, -3.1521e-01,\n",
            "         -1.1926e+00,  1.6574e+00, -6.2628e-01, -1.9368e-01, -8.4084e-01,\n",
            "         -2.4861e+00,  6.5146e-01, -3.5079e-01,  2.2157e+00, -6.6029e-01,\n",
            "         -1.9073e+00, -1.0114e+00,  1.1050e+00, -8.1991e-01, -3.9721e-01,\n",
            "          2.8139e+00,  1.6384e+00, -2.1341e-01, -1.4888e+00, -2.8686e-01,\n",
            "          6.7719e-01, -1.1023e+00,  1.2083e+00,  2.4406e+00,  4.9429e-01,\n",
            "          2.2916e-01,  5.7778e-01, -5.4317e-01,  2.7907e-01,  1.0884e+00,\n",
            "          2.0001e+00,  9.7330e-02, -4.3688e-01,  2.6285e+00, -1.2303e+00,\n",
            "         -6.9101e-02, -2.1221e-01, -6.7707e-02,  6.3978e-01, -5.6921e-01,\n",
            "          1.3897e+00, -1.3766e+00, -1.9243e+00,  1.1402e+00,  1.9068e+00,\n",
            "          1.2826e+00, -5.4423e-01, -1.7121e+00,  1.1939e+00, -1.7205e+00,\n",
            "         -1.7775e+00, -8.6316e-02, -1.6410e+00, -3.4080e-01, -1.6490e+00,\n",
            "          9.0411e-01,  3.4938e-01,  5.5876e-01, -1.0568e+00,  1.2577e+00,\n",
            "          4.5111e-01,  7.7384e-01,  9.6980e-01, -2.9943e+00,  2.0058e+00,\n",
            "         -1.0747e+00,  2.1247e+00, -6.9919e-02, -5.0132e-01, -1.4537e+00,\n",
            "          1.2728e-01,  3.8691e-01,  4.5830e-01,  1.4985e+00, -1.9109e-01],\n",
            "        [-1.6739e+00, -9.5838e-01, -2.1122e+00,  1.6328e-02,  6.9184e-01,\n",
            "         -4.8991e-02,  8.2236e-01,  3.3690e-01,  8.5309e-01,  1.0379e+00,\n",
            "          4.4310e-01, -1.0104e+00, -1.7840e+00, -1.4585e+00,  7.1882e-01,\n",
            "          1.8807e+00,  9.5836e-01,  2.9407e-01, -4.6355e-01,  7.7032e-01,\n",
            "          1.0528e-02, -1.9317e+00,  1.1221e+00, -3.0925e-03, -4.0000e-01,\n",
            "         -1.3633e+00, -6.1387e-01, -3.8415e-01,  7.1448e-01,  1.4688e+00,\n",
            "         -2.0320e+00,  1.3971e+00, -1.2822e+00,  8.4781e-01, -1.8025e+00,\n",
            "         -1.0971e-01, -1.0150e+00, -9.6700e-01, -1.7070e+00, -5.4675e-01,\n",
            "         -1.5278e+00,  2.6013e-01,  1.8424e+00,  4.9863e-01,  8.8037e-01,\n",
            "         -6.8958e-01,  1.1140e+00, -7.3096e-01,  1.1967e+00,  3.6725e-01,\n",
            "         -2.0018e-01,  6.1176e-01, -2.0039e+00, -2.6448e-01, -1.6243e+00,\n",
            "          6.0072e-01, -4.8283e-01, -3.9026e-01, -1.2404e+00,  1.9002e-02,\n",
            "         -1.0248e-03, -4.5401e-01, -1.6844e+00,  4.2587e-02, -1.6631e+00,\n",
            "          7.9157e-01,  2.4624e+00, -3.4970e-01, -4.6641e-01, -5.4963e-02,\n",
            "          2.2090e-01, -2.2050e-01, -1.4171e+00,  1.3069e+00, -1.6276e+00,\n",
            "          9.7872e-02,  1.2752e-02, -1.1739e-01,  3.7998e-02, -1.5037e+00,\n",
            "         -7.8742e-01, -1.4099e+00, -1.0362e+00,  1.3794e+00,  4.8735e-01,\n",
            "          4.7786e-01,  5.4757e-01, -1.3803e-01,  2.1594e-01, -2.9456e-01,\n",
            "          5.0165e-01, -5.2581e-01, -1.8247e+00, -5.3218e-02, -1.8125e+00,\n",
            "         -8.5463e-01,  6.8277e-01,  1.4419e+00,  6.8182e-01, -5.6930e-02],\n",
            "        [-5.6801e-01,  5.3103e-01, -7.8950e-01, -1.5397e+00,  1.0071e+00,\n",
            "         -2.1926e-01,  1.0554e+00, -3.8332e-01,  9.2574e-01, -5.1448e-01,\n",
            "          8.1664e-01, -8.8475e-01, -1.2680e+00,  1.8603e+00, -5.5821e-01,\n",
            "          9.9742e-01,  1.6495e+00,  5.3910e-01, -1.8009e+00, -2.8627e-01,\n",
            "          1.0915e+00,  8.8408e-02, -1.7930e+00,  1.0214e+00,  2.0144e-01,\n",
            "         -1.0540e+00,  1.4767e+00, -3.5190e+00,  1.1004e-01, -2.9626e-01,\n",
            "         -5.9924e-01,  3.0448e-01,  8.1793e-01,  1.9833e+00, -1.5640e+00,\n",
            "         -2.6113e-01, -1.2853e+00, -1.9593e+00, -2.6947e+00,  3.5663e-01,\n",
            "          1.1759e+00, -3.7055e-01,  5.3335e-01, -6.4487e-01, -2.2950e-01,\n",
            "         -8.6987e-01,  2.1637e+00, -1.1894e+00,  4.1314e-01,  5.9728e-01,\n",
            "          2.0535e-01,  8.0231e-01, -1.5617e+00,  4.9819e-01,  1.1948e+00,\n",
            "         -7.3850e-01,  1.3604e+00, -3.8446e-01,  4.5287e-01,  7.0866e-01,\n",
            "          5.0834e-01, -2.3464e+00, -1.2183e-01, -1.0701e+00,  1.3762e+00,\n",
            "          9.8755e-01,  7.6808e-01, -1.2076e+00,  3.3667e-01,  8.9684e-01,\n",
            "          1.6697e+00,  8.1081e-02, -8.6765e-01,  7.1748e-01, -1.7631e+00,\n",
            "          2.1769e-01,  1.4178e+00,  3.2085e-02, -7.2599e-01, -5.3471e-01,\n",
            "         -7.2483e-01, -1.3901e+00, -8.8113e-01, -1.0310e+00, -8.0575e-01,\n",
            "          7.0243e-01, -4.2638e-01, -1.3027e+00,  2.7496e-01,  1.1782e+00,\n",
            "         -9.2923e-01,  8.1835e-01,  6.1798e-02,  2.0812e-01, -9.0703e-01,\n",
            "         -1.8076e+00, -4.7793e-01,  2.8603e-02,  2.2157e-01, -1.6939e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "torch.Size([4, 100])\n",
            "Not train contras\n",
            "tensor([[ 1.1598, -0.4536, -1.4216, -0.9982,  0.2758,  0.9335,  0.1549,  1.1909,\n",
            "          0.7839,  1.5676, -0.1524,  0.6215, -1.1942, -0.5811,  0.6478,  1.6153,\n",
            "          0.2745, -1.0599, -1.5858,  0.2627,  0.3718, -0.7447,  0.5132,  0.2951,\n",
            "          1.3398, -0.0320, -2.4438, -1.2056,  1.5641,  0.1891, -2.1284, -2.4481,\n",
            "          0.8698, -0.3964, -0.8440, -0.6138, -1.6043,  1.6375, -1.7647,  0.4253,\n",
            "         -1.2630,  0.2728, -0.4985, -0.1485,  0.9940,  0.2105,  0.1751, -0.2545,\n",
            "         -0.8695, -0.2852,  0.0879,  0.8312, -0.9560,  1.3486,  0.6065,  0.5318,\n",
            "         -0.9036, -2.9497,  1.6832,  0.5485, -0.1787, -0.5971, -1.0025,  1.4013,\n",
            "         -1.7199, -2.0723,  0.2728, -1.3822, -1.0594, -0.5913,  1.9958,  0.2516,\n",
            "          0.5693,  1.0256,  0.9764, -0.3720,  0.9814, -0.1620, -2.8737, -0.3006,\n",
            "         -0.2973, -2.8766,  0.1896, -0.6898, -2.4868, -0.8784,  0.1001, -0.1425,\n",
            "         -0.8170,  2.9067, -0.6664, -0.0468, -1.0941, -0.1448, -0.2780, -1.2687,\n",
            "          0.6592,  0.1158,  1.4054,  0.4209],\n",
            "        [-0.0075,  0.3446, -1.5413,  2.8680,  2.2316,  0.2116, -0.9540,  0.7787,\n",
            "          1.7856,  0.9676,  1.0811,  0.0908, -0.8300,  0.9304,  0.3034,  0.4945,\n",
            "          0.3600, -0.5559,  0.4749,  1.1202, -1.0243, -0.4516, -0.6939,  0.8963,\n",
            "         -0.1989, -0.0555, -1.5247, -0.6248, -0.3495, -0.6708,  0.3523, -0.3958,\n",
            "          0.3367,  0.8717, -0.7682, -0.6449, -0.6701,  0.8107, -0.8572,  0.0102,\n",
            "          0.1277, -0.5411,  0.7005, -1.3885, -1.3926,  0.9580, -0.8005, -0.5872,\n",
            "          1.2921,  0.0430,  0.6158, -0.2434, -0.2045, -0.3456, -1.0764,  0.0712,\n",
            "          2.2957, -2.0934,  0.8771, -2.2718,  1.0381, -0.7816,  2.1090,  0.2031,\n",
            "          0.8174,  1.1377,  0.6557, -1.3080, -0.8995,  1.3504,  0.9049, -0.3409,\n",
            "         -0.6747,  0.8870, -2.5888,  0.5933,  1.8134,  0.6650, -1.8359, -0.2043,\n",
            "          0.4747,  0.4070,  0.5069,  1.6271,  0.2768, -0.8568, -0.8450, -1.5598,\n",
            "         -1.0170,  2.2385, -1.7254, -0.1986, -1.1706, -0.9591,  0.0628,  0.0385,\n",
            "         -0.1227,  0.9366,  0.5524, -1.7118],\n",
            "        [-1.4502, -0.1695,  0.2902,  0.3664,  0.3680, -0.5917, -0.2982, -1.2599,\n",
            "          0.6697,  0.4185, -0.0997, -0.7515,  1.1504,  1.3561,  0.7511,  0.5190,\n",
            "          1.6920, -1.5343, -1.7715, -0.1866,  0.8204, -0.3824, -0.2741,  0.0289,\n",
            "         -0.2207,  0.2598,  0.8807, -0.8965,  0.4735, -1.5530, -0.5228,  1.9934,\n",
            "          0.6534,  0.5992, -0.6891,  0.0211, -1.0221,  0.5906, -1.3090,  1.3880,\n",
            "          0.2903, -0.8589,  0.6387, -0.9725,  0.4741,  1.4257, -0.1839,  0.6009,\n",
            "          0.8674, -0.5544,  0.4823,  1.3758, -0.6965,  1.0446, -2.2803,  0.1615,\n",
            "         -0.1930,  0.1710,  2.0563, -2.5601,  0.3372, -1.1155,  0.7402, -0.3399,\n",
            "          1.3123,  0.6109,  0.9670, -0.0355,  0.3132,  1.2004,  1.8703, -1.0869,\n",
            "         -1.2939,  0.5904,  0.6429,  0.3687,  1.8989,  0.6036, -0.1988,  0.8933,\n",
            "         -1.1771, -0.9301, -0.7369,  1.0125, -1.6293, -0.3669,  1.4287, -0.2868,\n",
            "         -0.6915, -0.4420, -0.2505,  0.0167, -0.7096, -2.0933, -0.0295,  0.0415,\n",
            "          0.4307,  0.0131,  0.8722,  0.3437],\n",
            "        [ 0.1142, -0.3909, -0.9588,  0.6668,  0.8586,  0.3718,  0.8183,  0.2231,\n",
            "          0.0508, -0.0293, -0.0469, -1.0969, -1.6940,  1.1697,  0.1349, -0.2715,\n",
            "          0.8270,  1.0111, -1.6259,  0.5786,  0.2318,  0.7802,  0.6947,  0.8644,\n",
            "          0.0334,  0.1126,  0.7420, -2.4753,  1.3084, -1.1536, -0.6226, -0.8901,\n",
            "          0.7774,  0.7764, -1.1687, -1.0841, -2.2861,  0.3162,  0.2358, -0.1817,\n",
            "          1.8046,  0.7782, -1.2210,  1.0064, -0.0132, -1.3570,  0.1154, -0.0257,\n",
            "          0.7840,  1.5005,  0.5238, -0.0064, -1.6479,  2.5091, -1.2310,  0.3844,\n",
            "          1.0125, -0.1335, -0.8898, -0.0677,  0.9516, -1.0631,  0.7838,  0.3528,\n",
            "         -0.4691,  2.7496, -1.0949, -3.2810,  1.3151, -1.3673,  1.1238, -0.0676,\n",
            "          0.0815, -1.5921, -0.8519,  0.7264, -0.9829,  1.2386,  0.0127,  0.7394,\n",
            "         -0.1871,  0.3800, -0.0995,  0.8082, -0.5689, -0.4306,  1.7004,  1.2116,\n",
            "         -0.0657,  2.8172, -0.7695,  0.4626, -0.4520, -0.0216, -0.3931,  0.4618,\n",
            "          0.2431, -0.2939,  0.8944, -0.8174]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([4, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1209414026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch no {} #######################\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_max_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtrain_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3805520418.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_max_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#for jigsaw ssl task\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Cexperiment_name = 'e1_js'\n",
        "    Cdataset_config = 'js_d1'\n",
        "    Cweight_decay = 5e-4 # 5e-4\n",
        "    Clr = 1e-2 # originally 1e-2\n",
        "    Cepochs = 15\n",
        "    Cbatch_size = 4 #32\n",
        "\n",
        "    dataset_path = \"/content/drive/MyDrive/COMP 6970 - Computer Vision/Final Project/extraimages_small\"\n",
        "\n",
        "    # Data files which will get referred\n",
        "    permuts_file_path = 'selected_permuts.npy'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = 'resnet_jigsaw_solver_{}_trained.pt'.format(Cexperiment_name)\n",
        "\n",
        "    all_file_paths = get_paths(dataset_path)\n",
        "\n",
        "    # Get validation files separate\n",
        "    train_file_paths, val_file_paths = train_test_split(all_file_paths, test_size=0.1, shuffle=True, random_state=3)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data transforms\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.RandomCrop((64, 64)),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "\n",
        "    if Cdataset_config == 'js_d1':\n",
        "        train_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [GetJigsawPuzzleDataset(train_file_paths, permuts_file_path,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind+9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [GetJigsawPuzzleDataset(val_file_paths, permuts_file_path,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind + 9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                 ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    else:\n",
        "        train_data_loader = DataLoader(\n",
        "            GetJigsawPuzzleDataset(train_file_paths, permuts_file_path, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            GetJigsawPuzzleDataset(val_file_paths, permuts_file_path, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    print(\"Loaders done\")\n",
        "    # Print sample batches that would be returned by the train_data_loader\n",
        "    dataiter = iter(train_data_loader)\n",
        "    X, y = dataiter.__next__()\n",
        "    print (X.size())\n",
        "    print (y.size())\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_outputs = 100#200\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    # If using Resnet18\n",
        "    model_to_train = resnet18(num_classes=num_outputs, siamese_deg=9)\n",
        "    print('Model ready')\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    print('Started training')\n",
        "    model_train_test_obj = JigsawModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    for epoch_no in range(epochs):\n",
        "        print(\"Epoch no {} #######################\".format(epoch_no))\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader = train_data_loader, val_data_loader = val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"Train loss {} \\n Val loss {} \\n Train Acc {} \\n Val Acc {}\".format(train_loss,val_loss,train_acc,val_acc))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvg2Vwjb0Djz"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w--w8SfV0Djz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PamM29zi0Djz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cGivjmg0Djz"
      },
      "source": [
        "# Downstream classification training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U77ExY9s0Djz"
      },
      "outputs": [],
      "source": [
        "\n",
        "#for downstream classification\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import transforms, utils, models\n",
        "torch.manual_seed(3)\n",
        "\n",
        "\n",
        "def visualize(sample_data_loader):\n",
        "\n",
        "    def imshow(img, mean=0.0, std=1.0):\n",
        "        \"\"\"\n",
        "        Parameters passed:\n",
        "        img: Image to display\n",
        "        mean: Mean that was subtracted while normalizing the images\n",
        "        std: Standard deviation that was used for division while normalizing the image\n",
        "        \"\"\"\n",
        "        img = img * std + mean  # unnormalize\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    dataiter = iter(sample_data_loader)\n",
        "    images, labels = dataiter.__next__()\n",
        "    imshow(utils.make_grid(images))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    Cbatch_size = 32\n",
        "    Cepochs = 20\n",
        "    Coptim = 'adam'\n",
        "    Clr = 1e-4\n",
        "    Cweight_decay = 5e-4\n",
        "    Cjigsaw_task_weights = 'resnet_jigsaw_solver_e1_js_trained.pt'\n",
        "    Cmodel_file_name = 'resnet_trained_for_classification.pt'\n",
        "    Cexperiment_name = 'e1_last_b'\n",
        "    Ctrain_imagenet_based = False\n",
        "    Ctrain_ssl_block_4_ft = True\n",
        "    Ctrain_ssl_block_3_ft = False\n",
        "    Ctrain_ssl_full_ft = False\n",
        "    Ctrain_wo_ssl = False\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    jigsaw_pre_trained_weights_path =  Cjigsaw_task_weights\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels = \\\n",
        "        get_train_test_file_paths_n_labels()\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_image_ids, val_image_ids, train_file_paths, val_file_paths, train_labels, val_labels = \\\n",
        "        split_train_into_train_val(train_image_ids, train_file_paths, train_labels, test_size=0.1)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "    train_data_loader = DataLoader(\n",
        "        ConcatDataset(\n",
        "            [GetDataset(train_file_paths, train_labels, def_data_transform),\n",
        "             GetDataset(train_file_paths, train_labels, hflip_data_transform),\n",
        "             GetDataset(train_file_paths, train_labels, darkness_jitter_transform),\n",
        "             GetDataset(train_file_paths, train_labels, lightness_jitter_transform),\n",
        "             GetDataset(train_file_paths, train_labels, rotations_transform),\n",
        "             GetDataset(train_file_paths, train_labels, all_in_transform)]\n",
        "        ),\n",
        "        batch_size = batch_size, shuffle = True, num_workers = 8\n",
        "    )\n",
        "    val_data_gen = GetDataset(val_file_paths, val_labels, def_data_transform)\n",
        "    val_data_loader = DataLoader(\n",
        "        val_data_gen, batch_size=batch_size, shuffle=True, num_workers=8\n",
        "    )\n",
        "    test_data_gen = GetDataset(test_file_paths, test_labels, def_data_transform)\n",
        "    test_data_loader = DataLoader(\n",
        "        test_data_gen, batch_size=batch_size, shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    # Visualize a batch of images\n",
        "    # visualize(train_data_loader)\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_classes = 5\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    if Ctrain_imagenet_based:\n",
        "        model_to_train = models.resnet18(pretrained=True)\n",
        "        model_to_train.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        model_to_train.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048, 5),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "        model_file_path = 'resnet_imagenet_based.pt'\n",
        "\n",
        "    elif Ctrain_wo_ssl:\n",
        "        model_to_train = resnet18(num_classes=num_classes, siamese_deg=None)\n",
        "        model_file_path = 'resnet_trained_from_scratch.pt'\n",
        "\n",
        "    else:\n",
        "        model_to_train = resnet18(num_classes=num_classes, siamese_deg=None)\n",
        "        model_to_train.fc = nn.Linear(2048 * 9, 100)  # 2048 is the last resnet layer output length which gets\n",
        "        # multiplied with degree of siamese net, which for jigsaw puzzle solving was 9\n",
        "\n",
        "        # Load state dict for pre trained model weights\n",
        "        model_to_train.load_state_dict(torch.load(jigsaw_pre_trained_weights_path))\n",
        "\n",
        "        # Redefine the last linear layer\n",
        "        model_to_train.fc = nn.Linear(2048, 5)\n",
        "\n",
        "        print('Model loaded successfully')\n",
        "\n",
        "        if Ctrain_ssl_block_4_ft:\n",
        "            model_file_path = 'resnet_trained_ssl_{}_last_a_ft.pt'.format(Cexperiment_name)\n",
        "            for name, param in model_to_train.named_parameters():\n",
        "                if name[:6] == 'layer4' or name in ['fc.0.weight', 'fc.0.bias']:\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "        elif Ctrain_ssl_block_3_ft:\n",
        "            model_file_path = 'resnet_trained_ssl_{}_last_b_ft.pt'.format(Cexperiment_name)\n",
        "            for name, param in model_to_train.named_parameters():\n",
        "                if name[:6] == 'layer4' or name[:6] == 'layer3' or name in ['fc.0.weight', 'fc.0.bias']:\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        else:\n",
        "            model_to_train.fc = nn.Linear(2048, 5)\n",
        "            model_file_path = 'resnet_trained_ssl_{}_full_ft.pt'.format(Cexperiment_name)\n",
        "\n",
        "\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    sgd_optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    adam_optimizer = optim.Adam(model_to_train.parameters(), lr=lr, weight_decay=weight_decay_const)\n",
        "\n",
        "    if Coptim == 'sgd':\n",
        "        optimizer = sgd_optimizer\n",
        "    else:\n",
        "        optimizer = adam_optimizer\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    model_train_test_obj = ModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    print('Training started')\n",
        "    for epoch_no in range(epochs):\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader=train_data_loader, val_data_loader=val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W23_wiUw0Dj0"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIGCtep_0Dj0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('e1_last_b_classification_loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44iHEyNR0Dj0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('e1_last_b_classification_acc.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTK-UlOv0Dj0"
      },
      "source": [
        "# Show confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E_RCccA0Dj0"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    pil_img = Image.open(path)\n",
        "    if pil_img.mode == \"L\":\n",
        "        return None\n",
        "    else:\n",
        "        return pil_img\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    Cmodel_name = 'resnet_trained_ssl_e1_last_b_last_b_ft.pt'\n",
        "    Ctest_compact_bilinear = True\n",
        "    Ctest_imagenet_based = False\n",
        "    Ctest_on = 'test'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = Cmodel_name\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels = \\\n",
        "        get_train_test_file_paths_n_labels()\n",
        "\n",
        "    train_image_ids, val_image_ids, train_file_paths, val_file_paths, train_labels, val_labels = \\\n",
        "        split_train_into_train_val(train_image_ids, train_file_paths, train_labels, test_size=0.1)\n",
        "\n",
        "    if Ctest_imagenet_based:\n",
        "        model_to_train = models.resnet18(pretrained=True)\n",
        "        model_to_train.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "        model_to_train.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048, 5),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "    else:\n",
        "        model_to_train = resnet18(num_classes=5, siamese_deg=None)\n",
        "\n",
        "    # Check if saved model exists, and load if it does.\n",
        "    if os.path.exists(model_file_path):\n",
        "        model_to_train.load_state_dict(torch.load(model_file_path))\n",
        "    model_to_train.to(device)\n",
        "\n",
        "    # Setup on which set evaluation is to be carried out\n",
        "    if Ctest_on == 'train':\n",
        "        eval_file_paths, eval_labels = train_file_paths, train_labels\n",
        "    elif Ctest_on == 'val':\n",
        "        eval_file_paths, eval_labels = val_file_paths, val_labels\n",
        "    else:\n",
        "        eval_file_paths, eval_labels = test_file_paths, test_labels\n",
        "\n",
        "    # Start evaluation\n",
        "    model_to_train.eval()\n",
        "    correct = 0\n",
        "    preds = []\n",
        "    for f, label in zip(eval_file_paths, eval_labels):\n",
        "        pil_img = pil_loader(f)\n",
        "        if pil_img is None:\n",
        "            preds.append(0)\n",
        "            continue\n",
        "        data = def_data_transform(pil_img)\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        data = Variable(data, volatile=True).to(device)\n",
        "        output = model_to_train(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "\n",
        "        x = pred.data #prediction\n",
        "        preds.append(x)\n",
        "\n",
        "        if x == label:\n",
        "            correct += 1\n",
        "\n",
        "    print (correct, len(eval_file_paths), correct * 100 / len(eval_file_paths))\n",
        "    preds = np.array(preds).astype(np.float64)\n",
        "    conf_mat = np.array(confusion_matrix(preds, eval_labels))\n",
        "    conf_df = pd.DataFrame(conf_mat)\n",
        "    conf_df.columns = np.arange(0,5)\n",
        "    conf_df.to_csv('confusion_matrix.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}