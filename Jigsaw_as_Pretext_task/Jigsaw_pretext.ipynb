{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7Ot1x70Djr"
      },
      "source": [
        "# Generate permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37yJnYM0Djt",
        "outputId": "76a9410a-b190-4cfb-e76a-cccb0f0de7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already performed count of iterations with pairs of jigsaw permutations 100\n",
            "Length of set of taken:  80\n",
            "No of iterations it took to build top - 100 permutations array = 118\n",
            "No of permutations 100\n",
            "Sample permutation 0\n",
            "[0 8 6 1 3 4 5 7 2]\n",
            "Sample permutation 1\n",
            "[2 4 3 7 8 6 5 0 1]\n",
            "Sample permutation 2\n",
            "[4 1 0 8 2 3 5 6 7]\n",
            "Sample permutation 3\n",
            "[3 8 0 5 6 2 4 1 7]\n",
            "Sample permutation 4\n",
            "[6 3 4 5 1 2 8 0 7]\n",
            "Sample permutation 5\n",
            "[2 8 6 3 4 1 7 0 5]\n",
            "Sample permutation 6\n",
            "[3 7 5 6 4 0 2 8 1]\n",
            "Sample permutation 7\n",
            "[5 8 2 3 1 7 0 6 4]\n",
            "Sample permutation 8\n",
            "[4 1 3 8 7 5 2 6 0]\n",
            "Sample permutation 9\n",
            "[2 0 6 5 8 4 7 1 3]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "\n",
        "# Build list of all possible permutations\n",
        "permuts_list = list(itertools.permutations(range(9)))\n",
        "permuts_array = np.array(permuts_list)\n",
        "no_permuts = len(permuts_list)\n",
        "\n",
        "\n",
        "# Take top x permutations which have max average hamming distance\n",
        "permuts_to_take = 100#200\n",
        "set_of_taken = set()\n",
        "cnt_iterations = 0\n",
        "while True:\n",
        "    cnt_iterations += 1\n",
        "    x = random.randint(1, no_permuts - 1)\n",
        "    y = random.randint(1, no_permuts - 1)\n",
        "    permut_1 = permuts_array[x]\n",
        "    permut_2 = permuts_array[y]\n",
        "    hd = hamming(permut_1, permut_2)\n",
        "\n",
        "    if hd > 0.9 and (not x in set_of_taken) and (not y in set_of_taken):\n",
        "        set_of_taken.add(x)\n",
        "        set_of_taken.add(y)\n",
        "\n",
        "        if len(set_of_taken) == permuts_to_take:\n",
        "            break\n",
        "\n",
        "    if cnt_iterations % 100 == 0:\n",
        "        print (\"Already performed count of iterations with pairs of jigsaw permutations\", cnt_iterations)\n",
        "        print (\"Length of set of taken: \",len(set_of_taken))\n",
        "\n",
        "print (\"No of iterations it took to build top - {} permutations array = {}\".format(permuts_to_take, cnt_iterations))\n",
        "print (\"No of permutations\", len(set_of_taken))\n",
        "\n",
        "\n",
        "# Build the array for selected permutation indices above\n",
        "selected_permuts = []\n",
        "for ind, perm_id in enumerate(set_of_taken):\n",
        "    if ind < 10:\n",
        "        print (\"Sample permutation {}\".format(ind))\n",
        "        print (permuts_array[perm_id])\n",
        "    selected_permuts.append(permuts_array[perm_id])\n",
        "\n",
        "selected_permuts = np.array(selected_permuts)\n",
        "np.save('selected_permuts.npy', selected_permuts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "N5DPoz-a0Djv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "def_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "hflip_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "darkness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 0.9]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lightness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[1.1, 1.5]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "rotations_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "all_in_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def crop_from_center(pil_image, new_h, new_w):\n",
        "\n",
        "    width, height = pil_image.size  # Get dimensions\n",
        "\n",
        "    left = (width - new_w) / 2\n",
        "    top = (height - new_h) / 2\n",
        "    right = (width + new_w) / 2\n",
        "    bottom = (height + new_h) / 2\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def get_nine_crops(pil_image):\n",
        "    \"\"\"\n",
        "    Get nine crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/3)\n",
        "\n",
        "    r_vals = [0, diff, 2 * diff]\n",
        "    c_vals = [0, diff, 2 * diff]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def split_train_into_train_val(train_file_ids, train_file_paths, train_labels, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Split train_file_paths and train_labels to train_file_paths, val_file_paths and\n",
        "    train_labels, val_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mapping between image_id and file_path\n",
        "    image_id_name_map = dict(zip(train_file_ids, train_file_paths))\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_file_ids, val_file_ids, train_labels, val_labels = train_test_split(\n",
        "        train_file_ids, train_labels, test_size=test_size, random_state=5, shuffle=True\n",
        "    )\n",
        "    train_file_paths = [image_id_name_map[image_id] for image_id in train_file_ids]\n",
        "    val_file_paths = [image_id_name_map[image_id] for image_id in val_file_ids]\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels\", len(train_labels))\n",
        "    print (\"Length of val files list\", len(val_file_paths))\n",
        "    print (\"Length of val labels\", len(val_labels))\n",
        "\n",
        "    return train_file_ids, val_file_ids, train_file_paths, val_file_paths, train_labels, val_labels\n",
        "\n",
        "def get_paths(data_dir):\n",
        "    file_paths_to_return = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_paths_to_return.append(data_dir+'/'+file)\n",
        "\n",
        "    return file_paths_to_return\n",
        "\n",
        "def get_train_test_file_paths_n_labels():\n",
        "    \"\"\"\n",
        "    Get array train_file_paths, train_labels, test_file_paths and test_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    #par_data_dir = 'train'\n",
        "    images_data_dir = 'train'\n",
        "    train_test_split_file = 'train_test_split.txt'\n",
        "    images_file = 'images.txt'\n",
        "    labels_file = 'image_class_labels.txt'\n",
        "\n",
        "    # Read the images_file which stores image-id and image-name mapping\n",
        "    image_file_id_df = pd.read_csv(images_file, sep=' ', header=None)\n",
        "    image_file_id_mat = image_file_id_df.values\n",
        "    image_id_name_map = dict(zip(image_file_id_mat[:, 0], image_file_id_mat[:, 1]))\n",
        "\n",
        "    # Read the train_test_split file which stores image-id and train-test split mapping\n",
        "    image_id_train_test_split_df = pd.read_csv(train_test_split_file, sep=' ', header=None)\n",
        "    image_id_train_test_split_mat = image_id_train_test_split_df.values\n",
        "    image_id_train_test_split_map = dict(zip(image_id_train_test_split_mat[:, 0],\n",
        "                                             image_id_train_test_split_mat[:, 1]))\n",
        "\n",
        "    # Read the image class labels file\n",
        "    image_id_label_df = pd.read_csv(labels_file, sep=' ', header=None)\n",
        "    image_id_label_mat = image_id_label_df.values\n",
        "    image_id_label_map = dict(zip(image_id_label_mat[:, 0], image_id_label_mat[:, 1]))\n",
        "\n",
        "    # Put together train_files train_labels test_files and test_labels lists\n",
        "    train_image_ids, test_image_ids = [], []\n",
        "    train_file_paths, test_file_paths = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "    for file_id in image_id_name_map.keys():\n",
        "        file_name = image_id_name_map[file_id]\n",
        "        is_train = image_id_train_test_split_map[file_id]\n",
        "        label = image_id_label_map[file_id] - 1  # To ensure labels start from 0\n",
        "\n",
        "        if is_train:\n",
        "            train_image_ids.append(file_id)\n",
        "            train_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            train_labels.append(label)\n",
        "        else:\n",
        "            test_image_ids.append(file_id)\n",
        "            test_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels list\", len(train_labels))\n",
        "    print (\"Length of test files list\", len(test_file_paths))\n",
        "    print (\"Length of test labels list\", len(test_labels))\n",
        "\n",
        "    return train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UIXFNq0Djw"
      },
      "source": [
        "# Generate Jigsaw from permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "8SFOvvMU0Djw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#from dataset_helpers import crop_from_center, get_nine_crops\n",
        "\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        'Initialization'\n",
        "        self.imgs = [(img_path, label) for img_path, label in zip(file_paths, labels)]\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            label = self.labels[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        tr_image = self.transform(pil_image)\n",
        "\n",
        "        return tr_image, label\n",
        "\n",
        "\n",
        "class GetJigsawPuzzleDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None):\n",
        "        'Initialization'\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
        "        self.range_permut_indices = range_permut_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        pil_image = pil_image.resize((256, 256))\n",
        "        pil_image = crop_from_center(pil_image, 225, 225)\n",
        "\n",
        "        # Get nine crops for the image\n",
        "        nine_crops = get_nine_crops(pil_image)\n",
        "\n",
        "        # Permut the 9 patches obtained from the image\n",
        "        if self.range_permut_indices:\n",
        "            permut_ind = random.randint(self.range_permut_indices[0], self.range_permut_indices[1])\n",
        "        else:\n",
        "            permut_ind = random.randint(0, len(self.permuts_avail) - 1)\n",
        "\n",
        "        permutation_config = self.permuts_avail[permut_ind]\n",
        "\n",
        "        permuted_patches_arr = [None] * 9\n",
        "        for crop_new_pos, crop in zip(permutation_config, nine_crops):\n",
        "            permuted_patches_arr[crop_new_pos] = crop\n",
        "\n",
        "        # Apply data transforms\n",
        "        tensor_patches = torch.zeros(9, 3, 64, 64)\n",
        "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
        "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
        "            tensor_patches[ind] = jigsaw_patch_tr\n",
        "\n",
        "        return tensor_patches, permut_ind\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "class JigsawPuzzleDatasetWithRotation(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        self.grid_size = 3  # 3x3 puzzle\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        w, h = img.size\n",
        "        patch_w, patch_h = w // self.grid_size, h // self.grid_size\n",
        "\n",
        "        # Split image into patches\n",
        "        patches = []\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                patch = img.crop((j*patch_w, i*patch_h, (j+1)*patch_w, (i+1)*patch_h))\n",
        "                patches.append(patch)\n",
        "\n",
        "        # Create permutation\n",
        "        perm = list(range(self.grid_size**2))\n",
        "        random.shuffle(perm)\n",
        "        shuffled_patches = [patches[i] for i in perm]\n",
        "\n",
        "        # Apply random rotations to each patch\n",
        "        rotations = []\n",
        "        rotated_patches = []\n",
        "        for patch in shuffled_patches:\n",
        "            rot = random.choice([0, 90, 180, 270])\n",
        "            rotations.append(rot // 90)  # store as 0,1,2,3\n",
        "            rotated_patches.append(patch.rotate(rot))\n",
        "\n",
        "        if self.transform:\n",
        "            rotated_patches = [self.transform(p) for p in rotated_patches]\n",
        "\n",
        "        # Stack patches into tensor: (9, C, H, W)\n",
        "        patch_tensor = torch.stack(rotated_patches, dim=0)\n",
        "        perm_tensor = torch.tensor(perm, dtype=torch.long)\n",
        "        rot_tensor = torch.tensor(rotations, dtype=torch.long)\n",
        "\n",
        "        # Return patches, permutation, and rotation\n",
        "        return patch_tensor, perm_tensor, rot_tensor\n"
      ],
      "metadata": {
        "id": "6VqPkNk7HQgb"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R30hrWv0Djx"
      },
      "source": [
        "# Defining Resnet model\n",
        "Credit: https://github.com/aniket03/self_supervised_bird_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "gMNaA-Q80Djx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None, siamese_deg=9, train_contrastive=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.siamese_deg = siamese_deg\n",
        "        self.train_contrastive = train_contrastive\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        if self.siamese_deg is None:\n",
        "            self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
        "        else:\n",
        "            self.fc = nn.Linear(2048 * block.expansion * self.siamese_deg, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_feature_vectors(self, input_batch):\n",
        "        # Each input_batch would be of shape (batch_size, color_channels, h, w)\n",
        "        x = self.conv1(input_batch)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_patch):\n",
        "\n",
        "        return self.get_feature_vectors(input_patch)\n",
        "\n",
        "        # Data returned by data loaders is of the shape (batch_size, no_patches, h_patch, w_patch)\n",
        "        # That's why named input to patches_batch\n",
        "\n",
        "        # if self.siamese_deg is None:\n",
        "        #     final_feat_vectors = self.get_feature_vectors(input_batch)\n",
        "        #     x = F.dropout(final_feat_vectors)\n",
        "        #     x = F.log_softmax(self.fc(x))\n",
        "        # elif not self.train_contrastive:\n",
        "        #     final_feat_vectors = None\n",
        "\n",
        "        #     for patch_ind in range(self.siamese_deg):\n",
        "        #         # Each patch_batch would be of shape (batch_size, color_channels, h_patch, w_patch)\n",
        "        #         print(input_batch.shape)\n",
        "        #         patch_batch = input_batch[:, patch_ind, :, :, :]\n",
        "        #         patch_batch_features = self.get_feature_vectors(patch_batch)\n",
        "\n",
        "        #         if patch_ind == 0:\n",
        "        #             final_feat_vectors = patch_batch_features\n",
        "        #         else:\n",
        "        #             final_feat_vectors = torch.cat([final_feat_vectors, patch_batch_features], dim=1)\n",
        "        #     x = F.dropout(final_feat_vectors)\n",
        "        #     x = F.log_softmax(self.fc(x))\n",
        "        # else:\n",
        "        #     q_img_batch = input_batch[:, 0, :, :, :]\n",
        "        #     p_img_batch = input_batch[:, 1, :, :, :]\n",
        "        #     n_img_batch = input_batch[:, 2, :, :, :]\n",
        "\n",
        "        #     q_img_batch_feats = self.get_feature_vectors(q_img_batch)\n",
        "        #     p_img_batch_feats = self.get_feature_vectors(p_img_batch)\n",
        "        #     n_img_batch_feats = self.get_feature_vectors(n_img_batch)\n",
        "\n",
        "        #     pos_sq_dist = torch.norm(q_img_batch_feats - p_img_batch_feats, p=2, dim=1) ** 2\n",
        "        #     neg_sq_dist = torch.norm(q_img_batch_feats - n_img_batch_feats, p=2, dim=1) ** 2\n",
        "\n",
        "        #     x = pos_sq_dist - neg_sq_dist\n",
        "\n",
        "        # return x\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    \"\"\"\n",
        "    return _resnet(BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------\n",
        "# Jigsaw + Rotation ResNet\n",
        "# ------------------------\n",
        "class JigsawResNetWithRotation(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, layers=[2,2,2,2], num_patches=9):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.backbone = ResNet(block, layers)\n",
        "        feat_dim = 512 * block.expansion * 2 * 2  # 2x2 avgpool flatten\n",
        "\n",
        "        # Separate heads for position (9 classes) and rotation (4 classes)\n",
        "        self.position_heads = nn.ModuleList([nn.Linear(feat_dim, num_patches) for _ in range(num_patches)])\n",
        "        self.rotation_heads = nn.ModuleList([nn.Linear(feat_dim, 4) for _ in range(num_patches)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, num_patches, 3, H, W)\n",
        "        pos_outputs = []\n",
        "        rot_outputs = []\n",
        "\n",
        "        # feat = self.backbone(x)\n",
        "        # pos_outputs.append(self.position_heads[i](feat))\n",
        "        # rot_outputs.append(self.rotation_heads[i](feat))\n",
        "\n",
        "        for i in range(self.num_patches):\n",
        "            patch = x[:, i, :, :, :]\n",
        "            feat = self.backbone(patch)\n",
        "            pos_outputs.append(self.position_heads[i](feat))\n",
        "            rot_outputs.append(self.rotation_heads[i](feat))\n",
        "        pos_tensor = torch.stack(pos_outputs, dim=1)  # (batch_size, num_patches, 9)\n",
        "        rot_tensor = torch.stack(rot_outputs, dim=1)  # (batch_size, num_patches, 4)\n",
        "        return pos_tensor, rot_tensor\n",
        "\n",
        "# ------------------------\n",
        "# Factory function\n",
        "# ------------------------\n",
        "def jigsaw_resnet18(num_patches=9):\n",
        "    return JigsawResNetWithRotation(BasicBlock, [2,2,2,2], num_patches)"
      ],
      "metadata": {
        "id": "lrJ-jRAQGcDn"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "hdr6VxdF0Djy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "def get_count_correct_preds(network_output, target):\n",
        "\n",
        "    output = network_output\n",
        "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "    pred.data = pred.data.view_as(target.data)\n",
        "    correct = target.eq(pred).sum().item()\n",
        "\n",
        "    return correct\n",
        "\n",
        "\n",
        "class ModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(ModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "            data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            output = self.network(data)\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n",
        "\n",
        "\n",
        "class JigsawModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(JigsawModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "        total_loss = 0\n",
        "        total_correct_pos = 0\n",
        "        total_correct_rot = 0\n",
        "        total_patches = 0\n",
        "\n",
        "        for batch_idx, (patches, perm, rot) in enumerate(train_data_loader):\n",
        "\n",
        "            patches, perm, rot = Variable(patches).to(self.device), Variable(perm).to(self.device), Variable(rot).to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            pos_out, rot_out = self.network(patches)\n",
        "\n",
        "\n",
        "            # Compute patch-wise cross entropy loss\n",
        "            loss_pos = sum(F.cross_entropy(pos_out[:, i, :], perm[:, i]) for i in range(9)) / 9\n",
        "            loss_rot = sum(F.cross_entropy(rot_out[:, i, :], rot[:, i]) for i in range(9)) / 9\n",
        "            loss = loss_pos + loss_rot\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * patches.size(0)\n",
        "\n",
        "            # Accuracy metrics\n",
        "            pred_pos = pos_out.argmax(dim=2)\n",
        "            pred_rot = rot_out.argmax(dim=2)\n",
        "            total_correct_pos += (pred_pos == perm).sum().item()\n",
        "            total_correct_rot += (pred_rot == rot).sum().item()\n",
        "            total_patches += patches.size(0) * 9\n",
        "\n",
        "            # Half point for each correct position and each correct rotation\n",
        "            correct += (total_correct_pos + total_correct_rot) / 2\n",
        "\n",
        "\n",
        "            # loss = F.nll_loss(output, target)\n",
        "            # loss.backward()\n",
        "\n",
        "            # optimizer.step()\n",
        "\n",
        "            # correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            cnt_batches += 1\n",
        "            del patches, perm, rot, pos_out, rot_out\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        total_correct_pos = 0\n",
        "        total_correct_rot = 0\n",
        "        total_patches = 0\n",
        "\n",
        "        for batch_idx, (patches, perm, rot) in enumerate(test_data_loader):\n",
        "\n",
        "            patches, perm, rot = Variable(patches).to(self.device), Variable(perm).to(self.device), Variable(rot).to(self.device)\n",
        "            pos_out, rot_out = self.network(patches)\n",
        "\n",
        "            # Compute patch-wise cross entropy loss\n",
        "            loss_pos = sum(F.cross_entropy(pos_out[:, i, :], perm[:, i]) for i in range(9)) / 9\n",
        "            loss_rot = sum(F.cross_entropy(rot_out[:, i, :], rot[:, i]) for i in range(9)) / 9\n",
        "            test_loss = loss_pos.item() + loss_rot.item()\n",
        "\n",
        "            # data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            # output = self.network(data)\n",
        "            # test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "\n",
        "            # Accuracy metrics\n",
        "            pred_pos = pos_out.argmax(dim=2)\n",
        "            pred_rot = rot_out.argmax(dim=2)\n",
        "            total_correct_pos += (pred_pos == perm).sum().item()\n",
        "            total_correct_rot += (pred_rot == rot).sum().item()\n",
        "            total_patches += patches.size(0) * 9\n",
        "\n",
        "            # Half point for each correct position and each correct rotation\n",
        "            correct += (total_correct_pos + total_correct_rot) / 2\n",
        "            # correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del patches, perm, rot, pos_out, rot_out\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlmHNV6o0Djy"
      },
      "source": [
        "# Jigsaw as pretext task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noZzi5Bw0Djy",
        "outputId": "dcb2237a-08b1-4509-a09a-62d0ed8aaab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders done\n",
            "torch.Size([2, 9, 3, 266, 400])\n",
            "torch.Size([2, 9])\n",
            "torch.Size([2, 9])\n",
            "Model ready\n",
            "Started training\n",
            "Epoch no 0 #######################\n",
            "\n",
            "After epoch 0 - Test set: Average loss: 3.6817, Accuracy: 1.5/1 (150%)\n",
            "\n",
            "\n",
            "After epoch 0 - Train set: Average loss: 3.6249, Accuracy: 7.0/4 (175%)\n",
            "\n",
            "Train loss 3.6248849630355835 \n",
            " Val loss 3.6816576719284058 \n",
            " Train Acc 1.75 \n",
            " Val Acc 1.5\n"
          ]
        }
      ],
      "source": [
        "#for jigsaw ssl task\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Cexperiment_name = 'e1_js'\n",
        "    Cdataset_config = 'js_d2'\n",
        "    Cweight_decay = 5e-4\n",
        "    Clr = 1e-2 # originally 1e-2\n",
        "    Cepochs = 1\n",
        "    Cbatch_size = 2\n",
        "    data_dir = \"extraimages\"\n",
        "    num_patches = 9\n",
        "\n",
        "    # Data files which will get referred\n",
        "    permuts_file_path = 'selected_permuts.npy'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = 'resnet_jigsaw_solver_{}_trained.pt'.format(Cexperiment_name)\n",
        "\n",
        "    all_file_paths = get_paths(data_dir)\n",
        "\n",
        "    # Get validation files separate\n",
        "    train_file_paths, val_file_paths = train_test_split(all_file_paths, test_size=0.1, shuffle=True, random_state=3)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data transforms\n",
        "    data_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop((64, 64)),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "\n",
        "\n",
        "    if Cdataset_config == 'js_d1':\n",
        "        train_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [JigsawPuzzleDatasetWithRotation(train_file_paths,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind+9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [JigsawPuzzleDatasetWithRotation(val_file_paths,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind + 9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                 ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    else:\n",
        "        train_data_loader = DataLoader(\n",
        "            JigsawPuzzleDatasetWithRotation(train_file_paths, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            JigsawPuzzleDatasetWithRotation(val_file_paths, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    print(\"Loaders done\")\n",
        "    # Print sample batches that would be returned by the train_data_loader\n",
        "    dataiter = iter(train_data_loader)\n",
        "    X,  perm_tensor, rot_tensor = dataiter.__next__()\n",
        "    print (X.size())\n",
        "    print (perm_tensor.size())\n",
        "    print (rot_tensor.size())\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_outputs = 100#200\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    # If using Resnet18\n",
        "    model_to_train = jigsaw_resnet18(num_patches=num_patches)\n",
        "    print('Model ready')\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    print('Started training')\n",
        "    model_train_test_obj = JigsawModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    for epoch_no in range(epochs):\n",
        "        print(\"Epoch no {} #######################\".format(epoch_no))\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader = train_data_loader, val_data_loader = val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"Train loss {} \\n Val loss {} \\n Train Acc {} \\n Val Acc {}\".format(train_loss,val_loss,train_acc,val_acc))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvg2Vwjb0Djz"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "w--w8SfV0Djz",
        "outputId": "6262bff1-4eee-4f1f-ab72-2b9c42cc7715"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PamM29zi0Djz",
        "outputId": "e5688238-8d0b-457b-d9eb-6053b74073bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.jpg')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}