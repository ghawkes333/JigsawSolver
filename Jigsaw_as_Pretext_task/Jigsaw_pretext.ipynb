{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jigsaw solver\n"
      ],
      "metadata": {
        "id": "a1TZOBhM0pjl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLA-NhdviTpN"
      },
      "source": [
        "# Generate permutations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jigsaw_puzzle_size = 4\n",
        "num_permuts_saved = 10000\n",
        "images_dir = \"extraimages\""
      ],
      "metadata": {
        "id": "wjgwzfjuoZD5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4vvH8PQ3x1u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Khk-czxiTpP"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "# Build list of permutations such that each position is equally likely for each piece\n",
        "piece_indicies = [i for i in range(jigsaw_puzzle_size ** 2)]\n",
        "num_permuts = num_permuts_saved\n",
        "permuts = np.zeros((num_permuts_saved, jigsaw_puzzle_size ** 2), dtype=np.long)\n",
        "permuts_hash = {}\n",
        "count = 0\n",
        "while count < num_permuts:\n",
        "  x = np.array(np.random.permutation(piece_indicies))\n",
        "  y = np.array(np.random.permutation(piece_indicies), dtype=np.int32)\n",
        "  hd = hamming(x, y) > 0.9\n",
        "  x_hashcode = np.sum([10**(-1 * i) * x[i] for i in range(len(x))])\n",
        "  y_hashcode = np.sum([10**(-1 * i) * y[i] for i in range(len(y))])\n",
        "  if hd > 0.9 and (not x_hashcode in permuts_hash) and (not y_hashcode in permuts_hash):\n",
        "    permuts[count] = x\n",
        "    permuts[count + 1] = y\n",
        "    permuts_hash[x_hashcode] = True\n",
        "    permuts_hash[y_hashcode] = True\n",
        "    count = count + 2\n",
        "\n",
        "\n",
        "\n",
        "# Build the array for selected permutation indices above\n",
        "np.save('selected_permuts.npy', permuts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xQj55rQfiTpQ"
      },
      "outputs": [],
      "source": [
        "from ctypes import ArgumentError\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "def_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "hflip_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "darkness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 0.9]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lightness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[1.1, 1.5]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "rotations_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "all_in_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def crop_from_center(pil_image, new_h, new_w):\n",
        "\n",
        "    width, height = pil_image.size  # Get dimensions\n",
        "\n",
        "    left = (width - new_w) / 2\n",
        "    top = (height - new_h) / 2\n",
        "    right = (width + new_w) / 2\n",
        "    bottom = (height + new_h) / 2\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def get_nine_crops(pil_image):\n",
        "    \"\"\"\n",
        "    Get nine crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/3)\n",
        "\n",
        "    r_vals = [0, diff, 2 * diff]\n",
        "    c_vals = [0, diff, 2 * diff]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def get_several_crops(pil_image, jig_size):\n",
        "    \"\"\"\n",
        "    Get several crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :param jig_size: number of rows and columns for jigsaw\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/jig_size)\n",
        "\n",
        "    r_vals = [i * diff for i in range(jig_size)]\n",
        "    c_vals = [i * diff for i in range(jig_size)]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def split_train_into_train_val(train_file_ids, train_file_paths, train_labels, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Split train_file_paths and train_labels to train_file_paths, val_file_paths and\n",
        "    train_labels, val_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mapping between image_id and file_path\n",
        "    image_id_name_map = dict(zip(train_file_ids, train_file_paths))\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_file_ids, val_file_ids, train_labels, val_labels = train_test_split(\n",
        "        train_file_ids, train_labels, test_size=test_size, random_state=5, shuffle=True\n",
        "    )\n",
        "    train_file_paths = [image_id_name_map[image_id] for image_id in train_file_ids]\n",
        "    val_file_paths = [image_id_name_map[image_id] for image_id in val_file_ids]\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels\", len(train_labels))\n",
        "    print (\"Length of val files list\", len(val_file_paths))\n",
        "    print (\"Length of val labels\", len(val_labels))\n",
        "\n",
        "    return train_file_ids, val_file_ids, train_file_paths, val_file_paths, train_labels, val_labels\n",
        "\n",
        "def get_paths():\n",
        "    data_dir = images_dir\n",
        "    file_paths_to_return = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_paths_to_return.append(data_dir+'/'+file)\n",
        "\n",
        "    if len(file_paths_to_return) == 0:\n",
        "      raise ArgumentError(\"Data was not found. Ensure that a data folder is present\")\n",
        "\n",
        "    return file_paths_to_return\n",
        "\n",
        "def get_train_test_file_paths_n_labels():\n",
        "    \"\"\"\n",
        "    Get array train_file_paths, train_labels, test_file_paths and test_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    #par_data_dir = 'train'\n",
        "    images_data_dir = 'train'\n",
        "    train_test_split_file = 'train_test_split.txt'\n",
        "    images_file = 'images.txt'\n",
        "    labels_file = 'image_class_labels.txt'\n",
        "\n",
        "    # Read the images_file which stores image-id and image-name mapping\n",
        "    image_file_id_df = pd.read_csv(images_file, sep=' ', header=None)\n",
        "    image_file_id_mat = image_file_id_df.values\n",
        "    image_id_name_map = dict(zip(image_file_id_mat[:, 0], image_file_id_mat[:, 1]))\n",
        "\n",
        "    # Read the train_test_split file which stores image-id and train-test split mapping\n",
        "    image_id_train_test_split_df = pd.read_csv(train_test_split_file, sep=' ', header=None)\n",
        "    image_id_train_test_split_mat = image_id_train_test_split_df.values\n",
        "    image_id_train_test_split_map = dict(zip(image_id_train_test_split_mat[:, 0],\n",
        "                                             image_id_train_test_split_mat[:, 1]))\n",
        "\n",
        "    # Read the image class labels file\n",
        "    image_id_label_df = pd.read_csv(labels_file, sep=' ', header=None)\n",
        "    image_id_label_mat = image_id_label_df.values\n",
        "    image_id_label_map = dict(zip(image_id_label_mat[:, 0], image_id_label_mat[:, 1]))\n",
        "\n",
        "    # Put together train_files train_labels test_files and test_labels lists\n",
        "    train_image_ids, test_image_ids = [], []\n",
        "    train_file_paths, test_file_paths = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "    for file_id in image_id_name_map.keys():\n",
        "        file_name = image_id_name_map[file_id]\n",
        "        is_train = image_id_train_test_split_map[file_id]\n",
        "        label = image_id_label_map[file_id] - 1  # To ensure labels start from 0\n",
        "\n",
        "        if is_train:\n",
        "            train_image_ids.append(file_id)\n",
        "            train_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            train_labels.append(label)\n",
        "        else:\n",
        "            test_image_ids.append(file_id)\n",
        "            test_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels list\", len(train_labels))\n",
        "    print (\"Length of test files list\", len(test_file_paths))\n",
        "    print (\"Length of test labels list\", len(test_labels))\n",
        "\n",
        "    return train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpAZGXKmiTpR"
      },
      "source": [
        "# Generate Jigsaw from permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sm_sOs8piTpR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms.functional import rotate\n",
        "\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        'Initialization'\n",
        "        self.imgs = [(img_path, label) for img_path, label in zip(file_paths, labels)]\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            label = self.labels[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        tr_image = self.transform(pil_image)\n",
        "\n",
        "        return tr_image, label\n",
        "\n",
        "\n",
        "class GetJigsawPuzzleDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None, jig_size=4):\n",
        "        'Initialization'\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
        "        self.jig_size = jig_size\n",
        "        if range_permut_indices != None:\n",
        "          self.range_permut_indices = range_permut_indices\n",
        "        else:\n",
        "          self.range_permut_indices = (0, len(self.permuts_avail) - 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        pil_image = pil_image.resize((256, 256))\n",
        "        new_size = 256 - (256 % self.jig_size) # Ensure that the number of pixels fits the number of jigsaw pieces\n",
        "        pil_image = crop_from_center(pil_image, new_size, new_size)\n",
        "\n",
        "        # Split image into tiles (patches)\n",
        "        crops = get_several_crops(pil_image, self.jig_size)\n",
        "\n",
        "        # Generate a rotation sequence\n",
        "        rot_config = np.random.randint(4, size=self.jig_size ** 2)\n",
        "\n",
        "        # Permut the patches obtained from the image\n",
        "        if self.range_permut_indices:\n",
        "            permut_ind = int(random.randint(self.range_permut_indices[0], self.range_permut_indices[1]))\n",
        "        else:\n",
        "            permut_ind = int(random.randint(0, len(self.permuts_avail) - 1))\n",
        "\n",
        "        permutation_config = self.permuts_avail[permut_ind]\n",
        "\n",
        "        permuted_patches_arr = [None] * self.jig_size * self.jig_size\n",
        "        for crop_new_pos, rotation, crop in zip(permutation_config, rot_config, crops):\n",
        "            if rotation > 0:\n",
        "                crop = rotate(crop, angle=int(90*rotation))\n",
        "            permuted_patches_arr[crop_new_pos] = crop\n",
        "\n",
        "        # Apply data transforms\n",
        "        tensor_patches = torch.zeros(self.jig_size ** 2, 3, 64, 64) # (self.jig_size ** 2, number channels, patch height, patch width)\n",
        "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
        "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
        "            tensor_patches[ind] = jigsaw_patch_tr\n",
        "\n",
        "        return tensor_patches, permutation_config, rot_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itXKzIqhiTpT"
      },
      "source": [
        "# Defining Resnet model\n",
        "Credit: https://github.com/aniket03/self_supervised_bird_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BEYNDi47iTpT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, jigsaw_size=3, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None, siamese_deg=9, train_contrastive=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.siamese_deg = siamese_deg\n",
        "        self.train_contrastive = train_contrastive\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        self.pos_head = nn.Linear(2048 * block.expansion, jigsaw_size ** 2)\n",
        "        self.rot_head = nn.Linear(2048 * block.expansion, 4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_feature_vectors(self, input_batch):\n",
        "        # Each input_batch would be of shape (batch_size, color_channels, h, w)\n",
        "        x = self.conv1(input_batch)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        B, N, C, H, W = input_batch.shape\n",
        "        patch_features = []\n",
        "\n",
        "        for i in range(N):\n",
        "            feat = self.get_feature_vectors(input_batch[:, i, :, :, :])\n",
        "            patch_features.append(feat)\n",
        "        patch_features = torch.stack(patch_features, dim=1)  # [B, 9, feat_dim]\n",
        "\n",
        "        # Predict position and rotation for each patch\n",
        "        pos_logits = self.pos_head(patch_features)  # [B, 9, 9]\n",
        "        rot_logits = self.rot_head(patch_features)  # [B, 9, 4]\n",
        "\n",
        "        return pos_logits, rot_logits\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    \"\"\"\n",
        "    return _resnet(BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O02gxDG-iTpU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "def get_count_correct_preds(network_output, target):\n",
        "\n",
        "    output = network_output\n",
        "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "    pred.data = pred.data.view_as(target.data)\n",
        "    correct = target.eq(pred).sum().item()\n",
        "\n",
        "    return correct\n",
        "\n",
        "\n",
        "class ModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(ModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "            data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            output = self.network(data)\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n",
        "\n",
        "\n",
        "class JigsawModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(JigsawModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        cnt_batches = 0\n",
        "        pos_correct = 0\n",
        "        rot_correct = 0\n",
        "        total_pred_pos = 0\n",
        "        total_pred_rot = 0\n",
        "        batch_size = 0\n",
        "\n",
        "        for batch_idx, (data, pos_vector, rot_vector) in enumerate(train_data_loader):\n",
        "\n",
        "            data, pos_vector, rot_vector = Variable(data).to(self.device), Variable(pos_vector).to(self.device), Variable(rot_vector).to(self.device)\n",
        "            batch_size = data.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pos_log, rot_log = self.network(data)\n",
        "\n",
        "            pos_pred = pos_log.argmax(dim=2)  # Shape (B, patches)\n",
        "            rot_pred = rot_log.argmax(dim=2)\n",
        "\n",
        "            pos_log = pos_log.permute(0, 2, 1) # Shape (B, patches, patches)\n",
        "            rot_log = rot_log.permute(0, 2, 1) # Shape (B, 4, patches)\n",
        "\n",
        "\n",
        "            loss_pos = F.cross_entropy(pos_log, pos_vector) # Shape (B, patches, 9)\n",
        "            loss_rot = F.cross_entropy(rot_log, rot_vector)\n",
        "            loss = loss_pos + loss_rot\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            pos_correct += pos_pred.eq(pos_vector).sum() # Single number\n",
        "            rot_correct += rot_pred.eq(rot_vector).sum() # Single number\n",
        "            total_pred_pos += pos_pred.shape[0] * pos_pred.shape[1]\n",
        "            total_pred_rot += rot_pred.shape[0] * rot_pred.shape[1]\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, pos_vector, rot_vector, pos_log, rot_log\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        correct = (pos_correct + rot_correct) / 2\n",
        "        total_pred = total_pred_pos * cnt_batches\n",
        "\n",
        "        train_acc = correct / (total_pred_pos + total_pred_rot)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Number of batches: {}, Batch size: {}, Average loss: {:.4f}, Total accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, cnt_batches, batch_size, train_loss, correct, total_pred,\n",
        "            100. * correct / total_pred))\n",
        "        print(f'Position accuracy: {pos_correct}/{total_pred}, Rotation accuracy: {rot_correct}/{total_pred}')\n",
        "\n",
        "\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        pos_correct = 0\n",
        "        rot_correct = 0\n",
        "        total_pred_pos = 0\n",
        "        total_pred_rot = 0\n",
        "        batch_size = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, pos_vector, rot_vector) in enumerate(test_data_loader):\n",
        "            data, pos_vector, rot_vector = Variable(data).to(self.device), Variable(pos_vector).to(self.device), Variable(rot_vector).to(self.device)\n",
        "            pos_log, rot_log = self.network(data)\n",
        "            batch_size = data.shape[0]\n",
        "\n",
        "            pos_log, rot_log = self.network(data)\n",
        "            pos_pred = pos_log.argmax(dim=2)  # Shape (B, patches)\n",
        "            rot_pred = rot_log.argmax(dim=2)\n",
        "            pos_log = pos_log.permute(0, 2, 1) # Shape (B, patches, patches)\n",
        "            rot_log = rot_log.permute(0, 2, 1) # Shape (B, 4, patches)\n",
        "            loss_pos = F.cross_entropy(pos_log, pos_vector).item() # Shape (B, patches, 9)\n",
        "            loss_rot = F.cross_entropy(rot_log, rot_vector).item()\n",
        "            test_loss += (loss_pos + loss_rot) / 2\n",
        "\n",
        "\n",
        "            pos_correct += pos_pred.eq(pos_vector).sum() # Single number\n",
        "            rot_correct += rot_pred.eq(rot_vector).sum() # Single number\n",
        "\n",
        "\n",
        "            total_pred_pos += pos_pred.shape[0] * pos_pred.shape[1]\n",
        "            total_pred_rot += rot_pred.shape[0] * rot_pred.shape[1]\n",
        "            cnt_batches += 1\n",
        "\n",
        "\n",
        "            del data, pos_vector, rot_vector, pos_log, rot_log\n",
        "\n",
        "        correct = (pos_correct + rot_correct) / 2\n",
        "        total_pred = total_pred_pos * cnt_batches\n",
        "\n",
        "        test_loss /= cnt_batches\n",
        "        test_acc = correct / total_pred\n",
        "        print('\\nAfter epoch {} - Test set: Number of batches: {}, Batch size: {}, Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, len(test_data_loader), batch_size, test_loss, correct, total_pred,\n",
        "            100. * correct / total_pred))\n",
        "\n",
        "        return  test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv25ckxKiTpV"
      },
      "source": [
        "# Jigsaw as pretext task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qB6edlUIiTpV",
        "outputId": "3feb64ef-c815-4295-a258-53e89c809260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders done\n",
            "torch.Size([32, 16, 3, 64, 64])\n",
            "torch.Size([32, 16])\n",
            "Model ready\n",
            "Started training\n",
            "Epoch no 0 #######################\n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0005\n",
            ")\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f78bcf8d820>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f78bd0bbdd0>\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1022050683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_max_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mtrain_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-854898046.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mloss_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_rot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#for jigsaw ssl task\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Cexperiment_name = 'e1_js'\n",
        "    Cdataset_config = 'js_d1'\n",
        "    Cweight_decay = 5e-4\n",
        "    Clr = 1e-2\n",
        "    Cepochs = 1\n",
        "    Cbatch_size = 32\n",
        "\n",
        "    # Data files which will get referred\n",
        "    permuts_file_path = 'selected_permuts.npy'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = 'resnet_jigsaw_solver_{}_trained.pt'.format(Cexperiment_name)\n",
        "\n",
        "    all_file_paths = get_paths()\n",
        "\n",
        "    # Get validation files separate\n",
        "    train_file_paths, val_file_paths = train_test_split(all_file_paths, test_size=0.1, shuffle=True, random_state=3)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data transforms for each individual tile\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.RandomCrop((64, 64)),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "    jig_size = 4\n",
        "\n",
        "    if Cdataset_config == 'js_d1':\n",
        "        n = jig_size**2 + 1\n",
        "        train_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [GetJigsawPuzzleDataset(train_file_paths, permuts_file_path,\n",
        "                                      transform=data_transform, jig_size=jig_size)\n",
        "                 for st_perm_ind in range(0, n**2, n)\n",
        "                ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [GetJigsawPuzzleDataset(val_file_paths, permuts_file_path,\n",
        "                                        transform=data_transform, jig_size=jig_size)\n",
        "                 for st_perm_ind in range(0, n**2, n)\n",
        "                 ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "    else:\n",
        "        train_data_loader = DataLoader(\n",
        "            GetJigsawPuzzleDataset(train_file_paths, permuts_file_path, transform=data_transform, jig_size=jig_size),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            GetJigsawPuzzleDataset(val_file_paths, permuts_file_path, transform=data_transform, jig_size=jig_size),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "    print(\"Loaders done\")\n",
        "    # Print sample batches that would be returned by the train_data_loader\n",
        "    dataiter = iter(train_data_loader)\n",
        "    X, y, r = dataiter.__next__() # Returns patches, positions, rotations\n",
        "    print (X.size())\n",
        "    print (y.size())\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_outputs = num_permuts_saved#200\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    # If using Resnet18\n",
        "    model_to_train = resnet18(num_classes=num_outputs, siamese_deg=jig_size**2, jigsaw_size=jigsaw_puzzle_size)\n",
        "    print('Model ready')\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    # scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True, min_lr=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    print('Started training')\n",
        "    model_train_test_obj = JigsawModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    for epoch_no in range(epochs):\n",
        "        print(\"Epoch no {} #######################\".format(epoch_no))\n",
        "        print(optimizer)\n",
        "        print(train_data_loader)\n",
        "        print(val_data_loader)\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader = train_data_loader, val_data_loader = val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"Train loss {} \\n Val loss {} \\n Train Acc {} \\n Val Acc {}\".format(train_loss,val_loss,train_acc,val_acc))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    print(f\"val_losses: {val_losses}\")\n",
        "    print(f\"type val loss: {type(val_losses)}\")\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_file_paths)"
      ],
      "metadata": {
        "id": "WrbRVKMY6qmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lbE1N98iTpV"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAuZaAkCiTpV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5VXGhp7iTpV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "num_outputs = num_permuts_saved\n",
        "siamese_deg = jigsaw_puzzle_size ** 2\n",
        "\n",
        "# Use same transforms as training\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomCrop((64, 64)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "data_loader = GetDataset([\"mountain_Solution.jpg\"], [None], transform=data_transform)\n",
        "jig_loader = GetJigsawPuzzleDataset([\"mountain_Solution.jpg\"], 'selected_permuts.npy', transform=data_transform, jig_size=jigsaw_puzzle_size)\n",
        "\n",
        "\n",
        "model = resnet18(num_classes=num_outputs, siamese_deg=siamese_deg, jigsaw_size=jigsaw_puzzle_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"resnet_jigsaw_solver_e1_js_trained.pt\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "X, y = jig_loader[0]\n",
        "X = X.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model(X)\n",
        "\n",
        "print(\"Output shape: \", output.shape)\n",
        "out_im = transforms.functional.to_pil_image(output)\n",
        "\n",
        "out_im.show()\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "t7bPiDL4x2ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqvSlGHSiTpW"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    pil_img = Image.open(path)\n",
        "    if pil_img.mode == \"L\":\n",
        "        return None\n",
        "    else:\n",
        "        return pil_img\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    Cmodel_name = 'resnet_trained_ssl_e1_last_b_last_b_ft.pt'\n",
        "    Ctest_compact_bilinear = True\n",
        "    Ctest_imagenet_based = False\n",
        "    Ctest_on = 'test'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = Cmodel_name\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels = \\\n",
        "        get_train_test_file_paths_n_labels()\n",
        "\n",
        "    train_image_ids, val_image_ids, train_file_paths, val_file_paths, train_labels, val_labels = \\\n",
        "        split_train_into_train_val(train_image_ids, train_file_paths, train_labels, test_size=0.1)\n",
        "\n",
        "    if Ctest_imagenet_based:\n",
        "        model_to_train = models.resnet18(pretrained=True)\n",
        "        model_to_train.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "        model_to_train.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048, 5),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "    else:\n",
        "        model_to_train = resnet18(num_classes=5, siamese_deg=None, jigsaw_size=jigsaw_puzzle_size)\n",
        "\n",
        "    # Check if saved model exists, and load if it does.\n",
        "    if os.path.exists(model_file_path):\n",
        "        model_to_train.load_state_dict(torch.load(model_file_path))\n",
        "    model_to_train.to(device)\n",
        "\n",
        "    # Setup on which set evaluation is to be carried out\n",
        "    if Ctest_on == 'train':\n",
        "        eval_file_paths, eval_labels = train_file_paths, train_labels\n",
        "    elif Ctest_on == 'val':\n",
        "        eval_file_paths, eval_labels = val_file_paths, val_labels\n",
        "    else:\n",
        "        eval_file_paths, eval_labels = test_file_paths, test_labels\n",
        "\n",
        "    # Start evaluation\n",
        "    model_to_train.eval()\n",
        "    correct = 0\n",
        "    preds = []\n",
        "    for f, label in zip(eval_file_paths, eval_labels):\n",
        "        pil_img = pil_loader(f)\n",
        "        if pil_img is None:\n",
        "            preds.append(0)\n",
        "            continue\n",
        "        data = def_data_transform(pil_img)\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        data = Variable(data, volatile=True).to(device)\n",
        "        output = model_to_train(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "\n",
        "        x = pred.data #prediction\n",
        "        preds.append(x)\n",
        "\n",
        "        if x == label:\n",
        "            correct += 1\n",
        "\n",
        "    print (correct, len(eval_file_paths), correct * 100 / len(eval_file_paths))\n",
        "    preds = np.array(preds).astype(np.float64)\n",
        "    conf_mat = np.array(confusion_matrix(preds, eval_labels))\n",
        "    conf_df = pd.DataFrame(conf_mat)\n",
        "    conf_df.columns = np.arange(0,5)\n",
        "    conf_df.to_csv('confusion_matrix.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}