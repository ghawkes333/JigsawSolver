{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7Ot1x70Djr"
      },
      "source": [
        "# Generate permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37yJnYM0Djt",
        "outputId": "f5d40015-b5aa-4b8a-ce48-466acad5c15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already performed count of iterations with pairs of jigsaw permutations 100\n",
            "Length of set of taken:  76\n",
            "No of iterations it took to build top - 100 permutations array = 125\n",
            "No of permutations 100\n",
            "Sample permutation 0\n",
            "[4 6 5 0 7 3 2 8 1]\n",
            "Sample permutation 1\n",
            "[5 8 1 6 0 3 7 4 2]\n",
            "Sample permutation 2\n",
            "[8 3 7 0 2 4 5 6 1]\n",
            "Sample permutation 3\n",
            "[7 5 4 6 0 1 8 3 2]\n",
            "Sample permutation 4\n",
            "[6 0 4 8 5 3 1 2 7]\n",
            "Sample permutation 5\n",
            "[3 5 1 0 2 7 4 6 8]\n",
            "Sample permutation 6\n",
            "[5 6 7 0 2 3 1 8 4]\n",
            "Sample permutation 7\n",
            "[7 2 0 6 8 1 3 5 4]\n",
            "Sample permutation 8\n",
            "[1 7 6 0 8 5 2 4 3]\n",
            "Sample permutation 9\n",
            "[6 3 2 1 0 5 8 7 4]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "\n",
        "# Build list of all possible permutations\n",
        "permuts_list = list(itertools.permutations(range(9)))\n",
        "permuts_array = np.array(permuts_list)\n",
        "no_permuts = len(permuts_list)\n",
        "\n",
        "\n",
        "# Take top x permutations which have max average hamming distance\n",
        "permuts_to_take = 100#200\n",
        "set_of_taken = set()\n",
        "cnt_iterations = 0\n",
        "while True:\n",
        "    cnt_iterations += 1\n",
        "    x = random.randint(1, no_permuts - 1)\n",
        "    y = random.randint(1, no_permuts - 1)\n",
        "    permut_1 = permuts_array[x]\n",
        "    permut_2 = permuts_array[y]\n",
        "    hd = hamming(permut_1, permut_2)\n",
        "\n",
        "    if hd > 0.9 and (not x in set_of_taken) and (not y in set_of_taken):\n",
        "        set_of_taken.add(x)\n",
        "        set_of_taken.add(y)\n",
        "\n",
        "        if len(set_of_taken) == permuts_to_take:\n",
        "            break\n",
        "\n",
        "    if cnt_iterations % 100 == 0:\n",
        "        print (\"Already performed count of iterations with pairs of jigsaw permutations\", cnt_iterations)\n",
        "        print (\"Length of set of taken: \",len(set_of_taken))\n",
        "\n",
        "print (\"No of iterations it took to build top - {} permutations array = {}\".format(permuts_to_take, cnt_iterations))\n",
        "print (\"No of permutations\", len(set_of_taken))\n",
        "\n",
        "\n",
        "# Build the array for selected permutation indices above\n",
        "selected_permuts = []\n",
        "for ind, perm_id in enumerate(set_of_taken):\n",
        "    if ind < 10:\n",
        "        print (\"Sample permutation {}\".format(ind))\n",
        "        print (permuts_array[perm_id])\n",
        "    selected_permuts.append(permuts_array[perm_id])\n",
        "\n",
        "selected_permuts = np.array(selected_permuts)\n",
        "np.save('selected_permuts.npy', selected_permuts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "N5DPoz-a0Djv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "def_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "hflip_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "darkness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 0.9]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lightness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[1.1, 1.5]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "rotations_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "all_in_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def crop_from_center(pil_image, new_h, new_w):\n",
        "\n",
        "    width, height = pil_image.size  # Get dimensions\n",
        "\n",
        "    left = (width - new_w) / 2\n",
        "    top = (height - new_h) / 2\n",
        "    right = (width + new_w) / 2\n",
        "    bottom = (height + new_h) / 2\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def get_nine_crops(pil_image):\n",
        "    \"\"\"\n",
        "    Get nine crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/3)\n",
        "\n",
        "    r_vals = [0, diff, 2 * diff]\n",
        "    c_vals = [0, diff, 2 * diff]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def split_train_into_train_val(train_file_ids, train_file_paths, train_labels, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Split train_file_paths and train_labels to train_file_paths, val_file_paths and\n",
        "    train_labels, val_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mapping between image_id and file_path\n",
        "    image_id_name_map = dict(zip(train_file_ids, train_file_paths))\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_file_ids, val_file_ids, train_labels, val_labels = train_test_split(\n",
        "        train_file_ids, train_labels, test_size=test_size, random_state=5, shuffle=True\n",
        "    )\n",
        "    train_file_paths = [image_id_name_map[image_id] for image_id in train_file_ids]\n",
        "    val_file_paths = [image_id_name_map[image_id] for image_id in val_file_ids]\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels\", len(train_labels))\n",
        "    print (\"Length of val files list\", len(val_file_paths))\n",
        "    print (\"Length of val labels\", len(val_labels))\n",
        "\n",
        "    return train_file_ids, val_file_ids, train_file_paths, val_file_paths, train_labels, val_labels\n",
        "\n",
        "def get_paths(data_dir):\n",
        "    file_paths_to_return = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_paths_to_return.append(data_dir+'/'+file)\n",
        "\n",
        "    return file_paths_to_return\n",
        "\n",
        "def get_train_test_file_paths_n_labels():\n",
        "    \"\"\"\n",
        "    Get array train_file_paths, train_labels, test_file_paths and test_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    #par_data_dir = 'train'\n",
        "    images_data_dir = 'train'\n",
        "    train_test_split_file = 'train_test_split.txt'\n",
        "    images_file = 'images.txt'\n",
        "    labels_file = 'image_class_labels.txt'\n",
        "\n",
        "    # Read the images_file which stores image-id and image-name mapping\n",
        "    image_file_id_df = pd.read_csv(images_file, sep=' ', header=None)\n",
        "    image_file_id_mat = image_file_id_df.values\n",
        "    image_id_name_map = dict(zip(image_file_id_mat[:, 0], image_file_id_mat[:, 1]))\n",
        "\n",
        "    # Read the train_test_split file which stores image-id and train-test split mapping\n",
        "    image_id_train_test_split_df = pd.read_csv(train_test_split_file, sep=' ', header=None)\n",
        "    image_id_train_test_split_mat = image_id_train_test_split_df.values\n",
        "    image_id_train_test_split_map = dict(zip(image_id_train_test_split_mat[:, 0],\n",
        "                                             image_id_train_test_split_mat[:, 1]))\n",
        "\n",
        "    # Read the image class labels file\n",
        "    image_id_label_df = pd.read_csv(labels_file, sep=' ', header=None)\n",
        "    image_id_label_mat = image_id_label_df.values\n",
        "    image_id_label_map = dict(zip(image_id_label_mat[:, 0], image_id_label_mat[:, 1]))\n",
        "\n",
        "    # Put together train_files train_labels test_files and test_labels lists\n",
        "    train_image_ids, test_image_ids = [], []\n",
        "    train_file_paths, test_file_paths = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "    for file_id in image_id_name_map.keys():\n",
        "        file_name = image_id_name_map[file_id]\n",
        "        is_train = image_id_train_test_split_map[file_id]\n",
        "        label = image_id_label_map[file_id] - 1  # To ensure labels start from 0\n",
        "\n",
        "        if is_train:\n",
        "            train_image_ids.append(file_id)\n",
        "            train_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            train_labels.append(label)\n",
        "        else:\n",
        "            test_image_ids.append(file_id)\n",
        "            test_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels list\", len(train_labels))\n",
        "    print (\"Length of test files list\", len(test_file_paths))\n",
        "    print (\"Length of test labels list\", len(test_labels))\n",
        "\n",
        "    return train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UIXFNq0Djw"
      },
      "source": [
        "# Generate Jigsaw from permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "8SFOvvMU0Djw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#from dataset_helpers import crop_from_center, get_nine_crops\n",
        "\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        'Initialization'\n",
        "        self.imgs = [(img_path, label) for img_path, label in zip(file_paths, labels)]\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            label = self.labels[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        tr_image = self.transform(pil_image)\n",
        "\n",
        "        return tr_image, label\n",
        "\n",
        "\n",
        "class GetJigsawPuzzleDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None):\n",
        "        'Initialization'\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
        "        self.range_permut_indices = range_permut_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        pil_image = pil_image.resize((256, 256))\n",
        "        pil_image = crop_from_center(pil_image, 225, 225)\n",
        "\n",
        "        # Get nine crops for the image\n",
        "        nine_crops = get_nine_crops(pil_image)\n",
        "\n",
        "        # Permut the 9 patches obtained from the image\n",
        "        if self.range_permut_indices:\n",
        "            permut_ind = random.randint(self.range_permut_indices[0], self.range_permut_indices[1])\n",
        "        else:\n",
        "            permut_ind = random.randint(0, len(self.permuts_avail) - 1)\n",
        "\n",
        "        permutation_config = self.permuts_avail[permut_ind]\n",
        "\n",
        "        permuted_patches_arr = [None] * 9\n",
        "        for crop_new_pos, crop in zip(permutation_config, nine_crops):\n",
        "            permuted_patches_arr[crop_new_pos] = crop\n",
        "\n",
        "        # Apply data transforms\n",
        "        tensor_patches = torch.zeros(9, 3, 64, 64)\n",
        "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
        "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
        "            tensor_patches[ind] = jigsaw_patch_tr\n",
        "\n",
        "        return tensor_patches, permut_ind\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "class JigsawPuzzleDatasetWithRotation(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        self.grid_size = 3  # 3x3 puzzle\n",
        "        self.num_patches = self.grid_size ** 2\n",
        "        self.rotation_list = random.choices([0, 90, 180, 270], k=len(image_paths * (self.grid_size ** 2)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        w, h = img.size\n",
        "        patch_w, patch_h = w // self.grid_size, h // self.grid_size\n",
        "\n",
        "        # Split image into patches\n",
        "        patches = []\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                patch = img.crop((j*patch_w, i*patch_h, (j+1)*patch_w, (i+1)*patch_h))\n",
        "                patches.append(patch)\n",
        "\n",
        "        # Create permutation\n",
        "        perm = list(range(self.num_patches))\n",
        "        random.shuffle(perm)\n",
        "        shuffled_patches = [patches[i] for i in perm]\n",
        "\n",
        "        # Apply random rotations to each patch\n",
        "        rotations = []\n",
        "        rotated_patches = []\n",
        "        for patch_idx, patch in enumerate(shuffled_patches):\n",
        "            rot = self.rotation_list[idx + patch_idx]\n",
        "            rotations.append(rot // 90)  # store as 0,1,2,3\n",
        "            rotated_patches.append(patch.rotate(rot))\n",
        "\n",
        "        if self.transform:\n",
        "            rotated_patches = [self.transform(p) for p in rotated_patches]\n",
        "\n",
        "        # Stack patches into tensor: (9, C, H, W)\n",
        "        patch_tensor = torch.stack(rotated_patches, dim=0)\n",
        "        perm_tensor = torch.tensor(perm, dtype=torch.long)\n",
        "        rot_tensor = torch.tensor(rotations, dtype=torch.long)\n",
        "\n",
        "        # Return patches, permutation, and rotation\n",
        "        return patch_tensor, perm_tensor, rot_tensor\n"
      ],
      "metadata": {
        "id": "6VqPkNk7HQgb"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R30hrWv0Djx"
      },
      "source": [
        "# Defining Resnet model\n",
        "Credit: https://github.com/aniket03/self_supervised_bird_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "gMNaA-Q80Djx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None, siamese_deg=9, train_contrastive=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.siamese_deg = siamese_deg\n",
        "        self.train_contrastive = train_contrastive\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        if self.siamese_deg is None:\n",
        "            self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
        "        else:\n",
        "            self.fc = nn.Linear(2048 * block.expansion * self.siamese_deg, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_feature_vectors(self, input_batch):\n",
        "        # Each input_batch would be of shape (batch_size, color_channels, h, w)\n",
        "        x = self.conv1(input_batch)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_patch):\n",
        "\n",
        "        return self.get_feature_vectors(input_patch)\n",
        "\n",
        "        # Data returned by data loaders is of the shape (batch_size, no_patches, h_patch, w_patch)\n",
        "        # That's why named input to patches_batch\n",
        "\n",
        "        # if self.siamese_deg is None:\n",
        "        #     final_feat_vectors = self.get_feature_vectors(input_batch)\n",
        "        #     x = F.dropout(final_feat_vectors)\n",
        "        #     x = F.log_softmax(self.fc(x))\n",
        "        # elif not self.train_contrastive:\n",
        "        #     final_feat_vectors = None\n",
        "\n",
        "        #     for patch_ind in range(self.siamese_deg):\n",
        "        #         # Each patch_batch would be of shape (batch_size, color_channels, h_patch, w_patch)\n",
        "        #         print(input_batch.shape)\n",
        "        #         patch_batch = input_batch[:, patch_ind, :, :, :]\n",
        "        #         patch_batch_features = self.get_feature_vectors(patch_batch)\n",
        "\n",
        "        #         if patch_ind == 0:\n",
        "        #             final_feat_vectors = patch_batch_features\n",
        "        #         else:\n",
        "        #             final_feat_vectors = torch.cat([final_feat_vectors, patch_batch_features], dim=1)\n",
        "        #     x = F.dropout(final_feat_vectors)\n",
        "        #     x = F.log_softmax(self.fc(x))\n",
        "        # else:\n",
        "        #     q_img_batch = input_batch[:, 0, :, :, :]\n",
        "        #     p_img_batch = input_batch[:, 1, :, :, :]\n",
        "        #     n_img_batch = input_batch[:, 2, :, :, :]\n",
        "\n",
        "        #     q_img_batch_feats = self.get_feature_vectors(q_img_batch)\n",
        "        #     p_img_batch_feats = self.get_feature_vectors(p_img_batch)\n",
        "        #     n_img_batch_feats = self.get_feature_vectors(n_img_batch)\n",
        "\n",
        "        #     pos_sq_dist = torch.norm(q_img_batch_feats - p_img_batch_feats, p=2, dim=1) ** 2\n",
        "        #     neg_sq_dist = torch.norm(q_img_batch_feats - n_img_batch_feats, p=2, dim=1) ** 2\n",
        "\n",
        "        #     x = pos_sq_dist - neg_sq_dist\n",
        "\n",
        "        # return x\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    \"\"\"\n",
        "    return _resnet(BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------\n",
        "# Jigsaw + Rotation ResNet\n",
        "# ------------------------\n",
        "class JigsawResNetWithRotation(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, layers=[2,2,2,2], num_patches=9):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.backbone = ResNet(block, layers)\n",
        "        feat_dim = 512 * block.expansion * 2 * 2  # 2x2 avgpool flatten\n",
        "\n",
        "        # Separate heads for position (9 classes) and rotation (4 classes)\n",
        "        self.position_heads = nn.ModuleList([nn.Linear(feat_dim, num_patches) for _ in range(num_patches)])\n",
        "        self.rotation_heads = nn.ModuleList([nn.Linear(feat_dim, 4) for _ in range(num_patches)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, num_patches, 3, H, W)\n",
        "        pos_outputs = []\n",
        "        rot_outputs = []\n",
        "\n",
        "        # feat = self.backbone(x)\n",
        "        # pos_outputs.append(self.position_heads[i](feat))\n",
        "        # rot_outputs.append(self.rotation_heads[i](feat))\n",
        "\n",
        "        for i in range(self.num_patches):\n",
        "            patch = x[:, i, :, :, :]\n",
        "            feat = self.backbone(patch)\n",
        "            pos_outputs.append(self.position_heads[i](feat))\n",
        "            rot_outputs.append(self.rotation_heads[i](feat))\n",
        "        pos_tensor = torch.stack(pos_outputs, dim=1)  # (batch_size, num_patches, 9)\n",
        "        rot_tensor = torch.stack(rot_outputs, dim=1)  # (batch_size, num_patches, 4)\n",
        "        return pos_tensor, rot_tensor\n",
        "\n",
        "# ------------------------\n",
        "# Factory function\n",
        "# ------------------------\n",
        "def jigsaw_resnet18(num_patches=9):\n",
        "    return JigsawResNetWithRotation(BasicBlock, [2,2,2,2], num_patches)"
      ],
      "metadata": {
        "id": "lrJ-jRAQGcDn"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "hdr6VxdF0Djy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "def get_count_correct_preds(network_output, target):\n",
        "\n",
        "    output = network_output\n",
        "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "    pred.data = pred.data.view_as(target.data)\n",
        "    correct = target.eq(pred).sum().item()\n",
        "\n",
        "    return correct\n",
        "\n",
        "\n",
        "class ModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(ModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "            data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            output = self.network(data)\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n",
        "\n",
        "\n",
        "class JigsawModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(JigsawModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "        total_loss = 0\n",
        "        total_correct_pos = 0\n",
        "        total_correct_rot = 0\n",
        "        total_patches = 0\n",
        "\n",
        "\n",
        "        for batch_idx, (patches, perm, rot) in enumerate(train_data_loader):\n",
        "\n",
        "            patches, perm, rot = Variable(patches).to(self.device), Variable(perm).to(self.device), Variable(rot).to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            pos_out, rot_out = self.network(patches)\n",
        "\n",
        "\n",
        "            # Compute patch-wise cross entropy loss\n",
        "            loss_pos = sum(F.cross_entropy(pos_out[:, i, :], perm[:, i]) for i in range(9)) / 9\n",
        "            loss_rot = sum(F.cross_entropy(rot_out[:, i, :], rot[:, i]) for i in range(9)) / 9\n",
        "            loss = loss_pos + loss_rot\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * patches.size(0)\n",
        "\n",
        "            # Accuracy metrics\n",
        "            pred_pos = pos_out.argmax(dim=2)\n",
        "            pred_rot = rot_out.argmax(dim=2)\n",
        "            total_correct_pos += (pred_pos == perm).sum().item()\n",
        "            total_correct_rot += (pred_rot == rot).sum().item()\n",
        "            total_patches += patches.size(0) * 9\n",
        "\n",
        "\n",
        "            # loss = F.nll_loss(output, target)\n",
        "            # loss.backward()\n",
        "\n",
        "            # optimizer.step()\n",
        "\n",
        "            # correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            cnt_batches += 1\n",
        "            del patches, perm, rot, pos_out, rot_out\n",
        "\n",
        "\n",
        "        # Half point for each correct position and each correct rotation\n",
        "        correct = (total_correct_pos + total_correct_rot) / 2\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / total_patches\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        total_correct_pos = 0\n",
        "        total_correct_rot = 0\n",
        "        total_patches = 0\n",
        "\n",
        "        for batch_idx, (patches, perm, rot) in enumerate(test_data_loader):\n",
        "\n",
        "            patches, perm, rot = Variable(patches).to(self.device), Variable(perm).to(self.device), Variable(rot).to(self.device)\n",
        "            pos_out, rot_out = self.network(patches)\n",
        "\n",
        "            # Compute patch-wise cross entropy loss\n",
        "            loss_pos = sum(F.cross_entropy(pos_out[:, i, :], perm[:, i]) for i in range(9)) / 9\n",
        "            loss_rot = sum(F.cross_entropy(rot_out[:, i, :], rot[:, i]) for i in range(9)) / 9\n",
        "            test_loss = loss_pos.item() + loss_rot.item()\n",
        "\n",
        "            # data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            # output = self.network(data)\n",
        "            # test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "\n",
        "            # Accuracy metrics\n",
        "            pred_pos = pos_out.argmax(dim=2)\n",
        "            pred_rot = rot_out.argmax(dim=2)\n",
        "            total_correct_pos += (pred_pos == perm).sum().item()\n",
        "            total_correct_rot += (pred_rot == rot).sum().item()\n",
        "            total_patches += patches.size(0) * 9\n",
        "\n",
        "            # Half point for each correct position and each correct rotation\n",
        "            correct += (total_correct_pos + total_correct_rot) / 2\n",
        "            # correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del patches, perm, rot, pos_out, rot_out\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlmHNV6o0Djy"
      },
      "source": [
        "# Jigsaw as pretext task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noZzi5Bw0Djy",
        "outputId": "416854ab-7ab0-4de3-9cf4-dfc46baa763f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready\n",
            "Started training\n",
            "Epoch no 0 #######################\n",
            "\n",
            "After epoch 0 - Test set: Average loss: 3.5659, Accuracy: 2.5/1 (250%)\n",
            "\n",
            "\n",
            "After epoch 0 - Train set: Average loss: 3.8725, Accuracy: 4.0/4 (100%)\n",
            "\n",
            "Train loss 3.8725342750549316 \n",
            " Val loss 3.565889596939087 \n",
            " Train Acc 0.1111111111111111 \n",
            " Val Acc 2.5\n"
          ]
        }
      ],
      "source": [
        "#for jigsaw ssl task\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Cexperiment_name = 'e1_js'\n",
        "    Cdataset_config = 'js_d2'\n",
        "    Cweight_decay = 5e-4\n",
        "    Clr = 1e-2 # originally 1e-2\n",
        "    Cepochs = 1\n",
        "    Cbatch_size = 2\n",
        "    data_dir = \"extraimages\"\n",
        "    num_patches = 9\n",
        "\n",
        "    # Data files which will get referred\n",
        "    permuts_file_path = 'selected_permuts.npy'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = 'resnet_jigsaw_solver_{}_trained.pt'.format(Cexperiment_name)\n",
        "\n",
        "    all_file_paths = get_paths(data_dir)\n",
        "\n",
        "    # Get validation files separate\n",
        "    train_file_paths, val_file_paths = train_test_split(all_file_paths, test_size=0.1, shuffle=True, random_state=3)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data transforms\n",
        "    data_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop((64, 64)),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "\n",
        "\n",
        "    if Cdataset_config == 'js_d1':\n",
        "        train_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [JigsawPuzzleDatasetWithRotation(train_file_paths,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind+9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [JigsawPuzzleDatasetWithRotation(val_file_paths,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind + 9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                 ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    else:\n",
        "        train_data_loader = DataLoader(\n",
        "            JigsawPuzzleDatasetWithRotation(train_file_paths, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            JigsawPuzzleDatasetWithRotation(val_file_paths, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    print(\"Loaders done\")\n",
        "    # Print sample batches that would be returned by the train_data_loader\n",
        "    # dataiter = iter(train_data_loader)\n",
        "    # X,  perm_tensor, rot_tensor = dataiter.__next__()\n",
        "    # print (X.size())\n",
        "    # print (perm_tensor.size())\n",
        "    # print (rot_tensor.size())\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_outputs = 100#200\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    # If using Resnet18\n",
        "    model_to_train = jigsaw_resnet18(num_patches=num_patches)\n",
        "    print('Model ready')\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    print('Started training')\n",
        "    model_train_test_obj = JigsawModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    for epoch_no in range(epochs):\n",
        "        print(\"Epoch no {} #######################\".format(epoch_no))\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader = train_data_loader, val_data_loader = val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"Train loss {} \\n Val loss {} \\n Train Acc {} \\n Val Acc {}\".format(train_loss,val_loss,train_acc,val_acc))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvg2Vwjb0Djz"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "w--w8SfV0Djz",
        "outputId": "de5063d6-f793-4311-abf0-f2a2e63fa2db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PamM29zi0Djz",
        "outputId": "99b8c686-9ca3-4feb-a3e8-b1fca15b5ead"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.jpg')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}