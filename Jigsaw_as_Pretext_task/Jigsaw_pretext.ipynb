{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7Ot1x70Djr"
      },
      "source": [
        "# Generate permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37yJnYM0Djt",
        "outputId": "667fdafb-b532-4fa5-8cb1-1dce53784bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already performed count of iterations with pairs of jigsaw permutations 100\n",
            "Length of set of taken:  66\n",
            "No of iterations it took to build top - 100 permutations array = 148\n",
            "No of permutations 100\n",
            "Sample permutation 0\n",
            "[7 0 3 6 1 4 2 8 5]\n",
            "Sample permutation 1\n",
            "[1 3 2 8 0 4 5 6 7]\n",
            "Sample permutation 2\n",
            "[2 0 3 1 6 5 7 4 8]\n",
            "Sample permutation 3\n",
            "[8 5 4 0 6 7 1 3 2]\n",
            "Sample permutation 4\n",
            "[4 2 7 8 3 6 5 0 1]\n",
            "Sample permutation 5\n",
            "[3 1 6 2 5 4 7 8 0]\n",
            "Sample permutation 6\n",
            "[7 2 6 5 4 1 3 8 0]\n",
            "Sample permutation 7\n",
            "[7 5 3 8 6 4 1 0 2]\n",
            "Sample permutation 8\n",
            "[6 1 5 0 4 8 2 3 7]\n",
            "Sample permutation 9\n",
            "[7 2 6 0 5 1 4 8 3]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "\n",
        "# Build list of all possible permutations\n",
        "permuts_list = list(itertools.permutations(range(9)))\n",
        "permuts_array = np.array(permuts_list)\n",
        "no_permuts = len(permuts_list)\n",
        "\n",
        "\n",
        "# Take top x permutations which have max average hamming distance\n",
        "permuts_to_take = 100#200\n",
        "set_of_taken = set()\n",
        "cnt_iterations = 0\n",
        "while True:\n",
        "    cnt_iterations += 1\n",
        "    x = random.randint(1, no_permuts - 1)\n",
        "    y = random.randint(1, no_permuts - 1)\n",
        "    permut_1 = permuts_array[x]\n",
        "    permut_2 = permuts_array[y]\n",
        "    hd = hamming(permut_1, permut_2)\n",
        "\n",
        "    if hd > 0.9 and (not x in set_of_taken) and (not y in set_of_taken):\n",
        "        set_of_taken.add(x)\n",
        "        set_of_taken.add(y)\n",
        "\n",
        "        if len(set_of_taken) == permuts_to_take:\n",
        "            break\n",
        "\n",
        "    if cnt_iterations % 100 == 0:\n",
        "        print (\"Already performed count of iterations with pairs of jigsaw permutations\", cnt_iterations)\n",
        "        print (\"Length of set of taken: \",len(set_of_taken))\n",
        "\n",
        "print (\"No of iterations it took to build top - {} permutations array = {}\".format(permuts_to_take, cnt_iterations))\n",
        "print (\"No of permutations\", len(set_of_taken))\n",
        "\n",
        "\n",
        "# Build the array for selected permutation indices above\n",
        "selected_permuts = []\n",
        "for ind, perm_id in enumerate(set_of_taken):\n",
        "    if ind < 10:\n",
        "        print (\"Sample permutation {}\".format(ind))\n",
        "        print (permuts_array[perm_id])\n",
        "    selected_permuts.append(permuts_array[perm_id])\n",
        "\n",
        "selected_permuts = np.array(selected_permuts)\n",
        "np.save('selected_permuts.npy', selected_permuts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "N5DPoz-a0Djv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "def_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "hflip_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "darkness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 0.9]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lightness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[1.1, 1.5]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "rotations_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "all_in_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def crop_from_center(pil_image, new_h, new_w):\n",
        "\n",
        "    width, height = pil_image.size  # Get dimensions\n",
        "\n",
        "    left = (width - new_w) / 2\n",
        "    top = (height - new_h) / 2\n",
        "    right = (width + new_w) / 2\n",
        "    bottom = (height + new_h) / 2\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def get_nine_crops(pil_image):\n",
        "    \"\"\"\n",
        "    Get nine crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/3)\n",
        "\n",
        "    r_vals = [0, diff, 2 * diff]\n",
        "    c_vals = [0, diff, 2 * diff]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def split_train_into_train_val(train_file_ids, train_file_paths, train_labels, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Split train_file_paths and train_labels to train_file_paths, val_file_paths and\n",
        "    train_labels, val_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mapping between image_id and file_path\n",
        "    image_id_name_map = dict(zip(train_file_ids, train_file_paths))\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_file_ids, val_file_ids, train_labels, val_labels = train_test_split(\n",
        "        train_file_ids, train_labels, test_size=test_size, random_state=5, shuffle=True\n",
        "    )\n",
        "    train_file_paths = [image_id_name_map[image_id] for image_id in train_file_ids]\n",
        "    val_file_paths = [image_id_name_map[image_id] for image_id in val_file_ids]\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels\", len(train_labels))\n",
        "    print (\"Length of val files list\", len(val_file_paths))\n",
        "    print (\"Length of val labels\", len(val_labels))\n",
        "\n",
        "    return train_file_ids, val_file_ids, train_file_paths, val_file_paths, train_labels, val_labels\n",
        "\n",
        "def get_paths(data_dir):\n",
        "    file_paths_to_return = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_paths_to_return.append(data_dir+'/'+file)\n",
        "\n",
        "    return file_paths_to_return\n",
        "\n",
        "def get_train_test_file_paths_n_labels():\n",
        "    \"\"\"\n",
        "    Get array train_file_paths, train_labels, test_file_paths and test_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    #par_data_dir = 'train'\n",
        "    images_data_dir = 'train'\n",
        "    train_test_split_file = 'train_test_split.txt'\n",
        "    images_file = 'images.txt'\n",
        "    labels_file = 'image_class_labels.txt'\n",
        "\n",
        "    # Read the images_file which stores image-id and image-name mapping\n",
        "    image_file_id_df = pd.read_csv(images_file, sep=' ', header=None)\n",
        "    image_file_id_mat = image_file_id_df.values\n",
        "    image_id_name_map = dict(zip(image_file_id_mat[:, 0], image_file_id_mat[:, 1]))\n",
        "\n",
        "    # Read the train_test_split file which stores image-id and train-test split mapping\n",
        "    image_id_train_test_split_df = pd.read_csv(train_test_split_file, sep=' ', header=None)\n",
        "    image_id_train_test_split_mat = image_id_train_test_split_df.values\n",
        "    image_id_train_test_split_map = dict(zip(image_id_train_test_split_mat[:, 0],\n",
        "                                             image_id_train_test_split_mat[:, 1]))\n",
        "\n",
        "    # Read the image class labels file\n",
        "    image_id_label_df = pd.read_csv(labels_file, sep=' ', header=None)\n",
        "    image_id_label_mat = image_id_label_df.values\n",
        "    image_id_label_map = dict(zip(image_id_label_mat[:, 0], image_id_label_mat[:, 1]))\n",
        "\n",
        "    # Put together train_files train_labels test_files and test_labels lists\n",
        "    train_image_ids, test_image_ids = [], []\n",
        "    train_file_paths, test_file_paths = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "    for file_id in image_id_name_map.keys():\n",
        "        file_name = image_id_name_map[file_id]\n",
        "        is_train = image_id_train_test_split_map[file_id]\n",
        "        label = image_id_label_map[file_id] - 1  # To ensure labels start from 0\n",
        "\n",
        "        if is_train:\n",
        "            train_image_ids.append(file_id)\n",
        "            train_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            train_labels.append(label)\n",
        "        else:\n",
        "            test_image_ids.append(file_id)\n",
        "            test_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels list\", len(train_labels))\n",
        "    print (\"Length of test files list\", len(test_file_paths))\n",
        "    print (\"Length of test labels list\", len(test_labels))\n",
        "\n",
        "    return train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UIXFNq0Djw"
      },
      "source": [
        "# Generate Jigsaw from permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "8SFOvvMU0Djw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#from dataset_helpers import crop_from_center, get_nine_crops\n",
        "\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        'Initialization'\n",
        "        self.imgs = [(img_path, label) for img_path, label in zip(file_paths, labels)]\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            label = self.labels[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        tr_image = self.transform(pil_image)\n",
        "\n",
        "        return tr_image, label\n",
        "\n",
        "\n",
        "class GetJigsawPuzzleDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None):\n",
        "        'Initialization'\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
        "        self.range_permut_indices = range_permut_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        pil_image = pil_image.resize((256, 256))\n",
        "        pil_image = crop_from_center(pil_image, 225, 225)\n",
        "\n",
        "        # Get nine crops for the image\n",
        "        nine_crops = get_nine_crops(pil_image)\n",
        "\n",
        "        # Permut the 9 patches obtained from the image\n",
        "        if self.range_permut_indices:\n",
        "            permut_ind = random.randint(self.range_permut_indices[0], self.range_permut_indices[1])\n",
        "        else:\n",
        "            permut_ind = random.randint(0, len(self.permuts_avail) - 1)\n",
        "\n",
        "        permutation_config = self.permuts_avail[permut_ind]\n",
        "\n",
        "        permuted_patches_arr = [None] * 9\n",
        "        for crop_new_pos, crop in zip(permutation_config, nine_crops):\n",
        "            permuted_patches_arr[crop_new_pos] = crop\n",
        "\n",
        "        # Apply data transforms\n",
        "        tensor_patches = torch.zeros(9, 3, 64, 64)\n",
        "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
        "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
        "            tensor_patches[ind] = jigsaw_patch_tr\n",
        "\n",
        "        return tensor_patches, permut_ind\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "class JigsawPuzzleDatasetWithRotation(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        self.grid_size = 3  # 3x3 puzzle\n",
        "        self.num_patches = self.grid_size ** 2\n",
        "        self.rotation_list = random.choices([0, 90, 180, 270], k=len(image_paths * (self.grid_size ** 2)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        w, h = img.size\n",
        "        patch_w, patch_h = w // self.grid_size, h // self.grid_size\n",
        "\n",
        "        # Split image into patches\n",
        "        patches = []\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                patch = img.crop((j*patch_w, i*patch_h, (j+1)*patch_w, (i+1)*patch_h))\n",
        "                patches.append(patch)\n",
        "\n",
        "        # Create permutation\n",
        "        perm = list(range(self.num_patches))\n",
        "        random.shuffle(perm)\n",
        "        shuffled_patches = [patches[i] for i in perm]\n",
        "\n",
        "        # Apply random rotations to each patch\n",
        "        rotations = []\n",
        "        rotated_patches = []\n",
        "        for patch_idx, patch in enumerate(shuffled_patches):\n",
        "            rot = self.rotation_list[idx + patch_idx]\n",
        "            rotations.append(rot // 90)  # store as 0,1,2,3\n",
        "            rotated_patches.append(patch.rotate(rot))\n",
        "\n",
        "        if self.transform:\n",
        "            rotated_patches = [self.transform(p) for p in rotated_patches]\n",
        "\n",
        "        # Stack patches into tensor: (9, C, H, W)\n",
        "        patch_tensor = torch.stack(rotated_patches, dim=0)\n",
        "        perm_tensor = torch.tensor(perm, dtype=torch.long)\n",
        "        rot_tensor = torch.tensor(rotations, dtype=torch.long)\n",
        "\n",
        "        # Return patches, permutation, and rotation\n",
        "        return patch_tensor, perm_tensor, rot_tensor\n"
      ],
      "metadata": {
        "id": "6VqPkNk7HQgb"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R30hrWv0Djx"
      },
      "source": [
        "# Defining Resnet model\n",
        "Credit: https://github.com/aniket03/self_supervised_bird_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "gMNaA-Q80Djx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None, siamese_deg=9, train_contrastive=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.siamese_deg = siamese_deg\n",
        "        self.train_contrastive = train_contrastive\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        if self.siamese_deg is None:\n",
        "            self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
        "        else:\n",
        "            self.fc = nn.Linear(2048 * block.expansion * self.siamese_deg, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_feature_vectors(self, input_batch):\n",
        "        # Each input_batch would be of shape (batch_size, color_channels, h, w)\n",
        "        x = self.conv1(input_batch)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_patch):\n",
        "\n",
        "        return self.get_feature_vectors(input_patch)\n",
        "\n",
        "        # Data returned by data loaders is of the shape (batch_size, no_patches, h_patch, w_patch)\n",
        "        # That's why named input to patches_batch\n",
        "\n",
        "        # if self.siamese_deg is None:\n",
        "        #     final_feat_vectors = self.get_feature_vectors(input_batch)\n",
        "        #     x = F.dropout(final_feat_vectors)\n",
        "        #     x = F.log_softmax(self.fc(x))\n",
        "        # elif not self.train_contrastive:\n",
        "        #     final_feat_vectors = None\n",
        "\n",
        "        #     for patch_ind in range(self.siamese_deg):\n",
        "        #         # Each patch_batch would be of shape (batch_size, color_channels, h_patch, w_patch)\n",
        "        #         print(input_batch.shape)\n",
        "        #         patch_batch = input_batch[:, patch_ind, :, :, :]\n",
        "        #         patch_batch_features = self.get_feature_vectors(patch_batch)\n",
        "\n",
        "        #         if patch_ind == 0:\n",
        "        #             final_feat_vectors = patch_batch_features\n",
        "        #         else:\n",
        "        #             final_feat_vectors = torch.cat([final_feat_vectors, patch_batch_features], dim=1)\n",
        "        #     x = F.dropout(final_feat_vectors)\n",
        "        #     x = F.log_softmax(self.fc(x))\n",
        "        # else:\n",
        "        #     q_img_batch = input_batch[:, 0, :, :, :]\n",
        "        #     p_img_batch = input_batch[:, 1, :, :, :]\n",
        "        #     n_img_batch = input_batch[:, 2, :, :, :]\n",
        "\n",
        "        #     q_img_batch_feats = self.get_feature_vectors(q_img_batch)\n",
        "        #     p_img_batch_feats = self.get_feature_vectors(p_img_batch)\n",
        "        #     n_img_batch_feats = self.get_feature_vectors(n_img_batch)\n",
        "\n",
        "        #     pos_sq_dist = torch.norm(q_img_batch_feats - p_img_batch_feats, p=2, dim=1) ** 2\n",
        "        #     neg_sq_dist = torch.norm(q_img_batch_feats - n_img_batch_feats, p=2, dim=1) ** 2\n",
        "\n",
        "        #     x = pos_sq_dist - neg_sq_dist\n",
        "\n",
        "        # return x\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    \"\"\"\n",
        "    return _resnet(BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------\n",
        "# Jigsaw + Rotation ResNet\n",
        "# ------------------------\n",
        "class JigsawResNetWithRotation(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, layers=[2,2,2,2], num_patches=9):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.backbone = ResNet(block, layers)\n",
        "        feat_dim = 512 * block.expansion * 2 * 2  # 2x2 avgpool flatten\n",
        "\n",
        "        # Separate heads for position (9 classes) and rotation (4 classes)\n",
        "        self.position_heads = nn.ModuleList([nn.Linear(feat_dim, num_patches) for _ in range(num_patches)])\n",
        "        self.rotation_heads = nn.ModuleList([nn.Linear(feat_dim, 4) for _ in range(num_patches)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, num_patches, 3, H, W)\n",
        "        pos_outputs = []\n",
        "        rot_outputs = []\n",
        "\n",
        "        # feat = self.backbone(x)\n",
        "        # pos_outputs.append(self.position_heads[i](feat))\n",
        "        # rot_outputs.append(self.rotation_heads[i](feat))\n",
        "\n",
        "        for i in range(self.num_patches):\n",
        "            patch = x[:, i, :, :, :]\n",
        "            feat = self.backbone(patch)\n",
        "            pos_outputs.append(self.position_heads[i](feat))\n",
        "            rot_outputs.append(self.rotation_heads[i](feat))\n",
        "        pos_tensor = torch.stack(pos_outputs, dim=1)  # (batch_size, num_patches, 9)\n",
        "        rot_tensor = torch.stack(rot_outputs, dim=1)  # (batch_size, num_patches, 4)\n",
        "        return pos_tensor, rot_tensor\n",
        "\n",
        "# ------------------------\n",
        "# Factory function\n",
        "# ------------------------\n",
        "def jigsaw_resnet18(num_patches=9):\n",
        "    return JigsawResNetWithRotation(BasicBlock, [2,2,2,2], num_patches)"
      ],
      "metadata": {
        "id": "lrJ-jRAQGcDn"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "hdr6VxdF0Djy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def get_count_correct_preds(network_output, target):\n",
        "\n",
        "    output = network_output\n",
        "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "    pred.data = pred.data.view_as(target.data)\n",
        "    correct = target.eq(pred).sum().item()\n",
        "\n",
        "    return correct\n",
        "\n",
        "\n",
        "class ModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(ModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "            data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            output = self.network(data)\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n",
        "\n",
        "\n",
        "class JigsawModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(JigsawModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "        total_loss = 0\n",
        "        total_correct_pos = 0\n",
        "        total_correct_rot = 0\n",
        "        total_patches = 0\n",
        "\n",
        "\n",
        "        for batch_idx, (patches, perm, rot) in enumerate(train_data_loader):\n",
        "            patches, perm, rot = Variable(patches).to(self.device), Variable(perm).to(self.device), Variable(rot).to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            pos_out, rot_out = self.network(patches)\n",
        "\n",
        "            # Compute patch-wise cross entropy loss\n",
        "            # loss_pos = sum(F.cross_entropy(pos_out[:, i, :], perm[:, i]) for i in range(9)) / 9\n",
        "            # loss_rot = sum(F.cross_entropy(rot_out[:, i, :], rot[:, i]) for i in range(9)) / 9\n",
        "\n",
        "            B, P, _ = pos_out.shape\n",
        "            pos_out_flat = pos_out.reshape(B*P, P)\n",
        "            perm_flat = perm.reshape(B*P)\n",
        "            rot_out_flat = rot_out.reshape(B*P, 4)\n",
        "            rot_flat = rot.reshape(B*P)\n",
        "            loss_pos = F.cross_entropy(pos_out_flat, perm_flat)\n",
        "            loss_rot = F.cross_entropy(rot_out_flat, rot_flat)\n",
        "            loss = loss_pos + loss_rot\n",
        "\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * patches.size(0)\n",
        "\n",
        "            # Accuracy metrics\n",
        "            pred_pos = pos_out.argmax(dim=2)\n",
        "            pred_rot = rot_out.argmax(dim=2)\n",
        "            total_correct_pos += (pred_pos == perm).sum().item()\n",
        "            total_correct_rot += (pred_rot == rot).sum().item()\n",
        "            total_patches += patches.size(0) * 9\n",
        "\n",
        "\n",
        "            # loss = F.nll_loss(output, target)\n",
        "            # loss.backward()\n",
        "\n",
        "            # optimizer.step()\n",
        "\n",
        "            # correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            cnt_batches += 1\n",
        "            del patches, perm, rot, pos_out, rot_out\n",
        "\n",
        "\n",
        "\n",
        "        # Half point for each correct position and each correct rotation\n",
        "        correct = (total_correct_pos + total_correct_rot) / 2\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / total_patches\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Position accuracy: {}/{} ({:.0f}%), Rotation: {}/{}({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, total_patches,\n",
        "            100. * train_acc, total_correct_pos, total_patches, 100*total_correct_pos/total_patches, total_correct_rot, total_patches, 100*total_correct_rot/total_patches))\n",
        "\n",
        "\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        total_correct_pos = 0\n",
        "        total_correct_rot = 0\n",
        "        total_patches = 0\n",
        "\n",
        "        for batch_idx, (patches, perm, rot) in enumerate(test_data_loader):\n",
        "\n",
        "            patches, perm, rot = Variable(patches).to(self.device), Variable(perm).to(self.device), Variable(rot).to(self.device)\n",
        "            pos_out, rot_out = self.network(patches)\n",
        "\n",
        "            # Compute patch-wise cross entropy loss\n",
        "            # loss_pos = sum(F.cross_entropy(pos_out[:, i, :], perm[:, i]) for i in range(9)) / 9\n",
        "            # loss_rot = sum(F.cross_entropy(rot_out[:, i, :], rot[:, i]) for i in range(9)) / 9\n",
        "            # test_loss = loss_pos.item() + loss_rot.item()\n",
        "            B, P, _ = pos_out.shape\n",
        "            pos_out_flat = pos_out.reshape(B*P, P)\n",
        "            perm_flat = perm.reshape(B*P)\n",
        "            loss_pos = F.cross_entropy(pos_out_flat, perm_flat).item()\n",
        "            rot_out_flat = rot_out.reshape(B*P, 4)\n",
        "            rot_flat = rot.reshape(B*P)\n",
        "            loss_rot = F.cross_entropy(rot_out_flat, rot_flat).item()\n",
        "            test_loss = (loss_pos + loss_rot)\n",
        "\n",
        "            # data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            # output = self.network(data)\n",
        "            # test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "\n",
        "            # Accuracy metrics\n",
        "            pred_pos = pos_out.argmax(dim=2)\n",
        "            pred_rot = rot_out.argmax(dim=2)\n",
        "            total_correct_pos += (pred_pos == perm).sum().item()\n",
        "            total_correct_rot += (pred_rot == rot).sum().item()\n",
        "            total_patches += patches.size(0) * 9\n",
        "\n",
        "            # Half point for each correct position and each correct rotation\n",
        "            correct += (total_correct_pos + total_correct_rot) / 2\n",
        "            # correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del patches, perm, rot, pos_out, rot_out\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / total_patches\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Position accuracy: {}/{} ({:.0f}%), Rotation: {}/{}({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, total_patches,\n",
        "            100. * test_acc, total_correct_pos, total_patches, 100*total_correct_pos/total_patches, total_correct_rot, total_patches, 100*total_correct_rot/total_patches))\n",
        "\n",
        "        return  test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlmHNV6o0Djy"
      },
      "source": [
        "# Jigsaw as pretext task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noZzi5Bw0Djy",
        "outputId": "dd771c5b-8d67-4126-d1c4-1b5ca20861d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready\n",
            "Started training\n",
            "Epoch no 0 #######################\n",
            "\n",
            "After epoch 0 - Test set: Average loss: 3.7698, Accuracy: 1.5/9 (17%), Position accuracy: 1/9 (11%), Rotation: 2/9(22%)\n",
            "\n",
            "\n",
            "After epoch 0 - Train set: Average loss: 3.7385, Accuracy: 7.0/36 (19%), Position accuracy: 1/36 (3%), Rotation: 13/36(36%)\n",
            "\n",
            "Train loss 3.738471031188965 \n",
            " Val loss 3.769779086112976 \n",
            " Train Acc 0.19444444444444445 \n",
            " Val Acc 0.16666666666666666\n"
          ]
        }
      ],
      "source": [
        "#for jigsaw ssl task\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Cexperiment_name = 'e1_js'\n",
        "    Cdataset_config = 'js_d2'\n",
        "    Cweight_decay = 5e-4\n",
        "    Clr = 1e-2 # originally 1e-2\n",
        "    Cepochs = 1\n",
        "    Cbatch_size = 2\n",
        "    data_dir = \"extraimages\"\n",
        "    num_patches = 9\n",
        "\n",
        "    # Data files which will get referred\n",
        "    permuts_file_path = 'selected_permuts.npy'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = 'resnet_jigsaw_solver_{}_trained.pt'.format(Cexperiment_name)\n",
        "\n",
        "    all_file_paths = get_paths(data_dir)\n",
        "\n",
        "    # Get validation files separate\n",
        "    train_file_paths, val_file_paths = train_test_split(all_file_paths, test_size=0.1, shuffle=True, random_state=3)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data transforms\n",
        "    data_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop((64, 64)),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "\n",
        "\n",
        "    if Cdataset_config == 'js_d1':\n",
        "        train_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [JigsawPuzzleDatasetWithRotation(train_file_paths,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind+9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [JigsawPuzzleDatasetWithRotation(val_file_paths,\n",
        "                                        range_permut_indices=[st_perm_ind, st_perm_ind + 9], transform=data_transform)\n",
        "                 for st_perm_ind in range(0, 100, 10)\n",
        "                 ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    else:\n",
        "        train_data_loader = DataLoader(\n",
        "            JigsawPuzzleDatasetWithRotation(train_file_paths, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            JigsawPuzzleDatasetWithRotation(val_file_paths, transform=data_transform),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=8\n",
        "        )\n",
        "    print(\"Loaders done\")\n",
        "    # Print sample batches that would be returned by the train_data_loader\n",
        "    # dataiter = iter(train_data_loader)\n",
        "    # X,  perm_tensor, rot_tensor = dataiter.__next__()\n",
        "    # print (X.size())\n",
        "    # print (perm_tensor.size())\n",
        "    # print (rot_tensor.size())\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_outputs = 100#200\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    # If using Resnet18\n",
        "    model_to_train = jigsaw_resnet18(num_patches=num_patches)\n",
        "    print('Model ready')\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    print('Started training')\n",
        "    model_train_test_obj = JigsawModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    for epoch_no in range(epochs):\n",
        "        print(\"Epoch no {} #######################\".format(epoch_no))\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader = train_data_loader, val_data_loader = val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"Train loss {} \\n Val loss {} \\n Train Acc {} \\n Val Acc {}\".format(train_loss,val_loss,train_acc,val_acc))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvg2Vwjb0Djz"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "w--w8SfV0Djz",
        "outputId": "c51912b9-3a33-43be-fd16-c8e45949867c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PamM29zi0Djz",
        "outputId": "7274de03-99fd-415d-f2c4-f7308ef8c644"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.jpg')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}