{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jigsaw solver\n"
      ],
      "metadata": {
        "id": "a1TZOBhM0pjl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLA-NhdviTpN"
      },
      "source": [
        "# Generate permutations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jigsaw_puzzle_size = 4\n",
        "num_permuts_saved = 10000\n",
        "images_dir = \"extraimages\""
      ],
      "metadata": {
        "id": "wjgwzfjuoZD5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4vvH8PQ3x1u"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "8Khk-czxiTpP"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "# Build list of permutations such that each position is equally likely for each piece\n",
        "piece_indicies = [i for i in range(jigsaw_puzzle_size ** 2)]\n",
        "num_permuts = num_permuts_saved\n",
        "permuts = np.zeros((num_permuts_saved, jigsaw_puzzle_size ** 2), dtype=np.long)\n",
        "permuts_hash = {}\n",
        "count = 0\n",
        "while count < num_permuts:\n",
        "  x = np.array(np.random.permutation(piece_indicies))\n",
        "  y = np.array(np.random.permutation(piece_indicies), dtype=np.int32)\n",
        "  hd = hamming(x, y) > 0.9\n",
        "  x_hashcode = np.sum([10**(-1 * i) * x[i] for i in range(len(x))])\n",
        "  y_hashcode = np.sum([10**(-1 * i) * y[i] for i in range(len(y))])\n",
        "  if hd > 0.9 and (not x_hashcode in permuts_hash) and (not y_hashcode in permuts_hash):\n",
        "    permuts[count] = x\n",
        "    permuts[count + 1] = y\n",
        "    permuts_hash[x_hashcode] = True\n",
        "    permuts_hash[y_hashcode] = True\n",
        "    count = count + 2\n",
        "\n",
        "\n",
        "\n",
        "# Build the array for selected permutation indices above\n",
        "np.save('selected_permuts.npy', permuts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "xQj55rQfiTpQ"
      },
      "outputs": [],
      "source": [
        "from ctypes import ArgumentError\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "def_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "hflip_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "darkness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 0.9]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lightness_jitter_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=[1.1, 1.5]),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "rotations_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "all_in_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def crop_from_center(pil_image, new_h, new_w):\n",
        "\n",
        "    width, height = pil_image.size  # Get dimensions\n",
        "\n",
        "    left = (width - new_w) / 2\n",
        "    top = (height - new_h) / 2\n",
        "    right = (width + new_w) / 2\n",
        "    bottom = (height + new_h) / 2\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def get_nine_crops(pil_image):\n",
        "    \"\"\"\n",
        "    Get nine crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/3)\n",
        "\n",
        "    r_vals = [0, diff, 2 * diff]\n",
        "    c_vals = [0, diff, 2 * diff]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def get_several_crops(pil_image, jig_size):\n",
        "    \"\"\"\n",
        "    Get several crops for a square pillow image. That is height and width of the image should be same.\n",
        "    :param pil_image: pillow image\n",
        "    :param jig_size: number of rows and columns for jigsaw\n",
        "    :return: List of pillow images. The nine crops\n",
        "    \"\"\"\n",
        "    w, h = pil_image.size\n",
        "    diff = int(w/jig_size)\n",
        "\n",
        "    r_vals = [i * diff for i in range(jig_size)]\n",
        "    c_vals = [i * diff for i in range(jig_size)]\n",
        "\n",
        "    list_patches = []\n",
        "\n",
        "    for r in r_vals:\n",
        "        for c in c_vals:\n",
        "\n",
        "            left = c\n",
        "            top = r\n",
        "            right = c + diff\n",
        "            bottom = r + diff\n",
        "\n",
        "            patch = pil_image.crop((left, top, right, bottom))\n",
        "            list_patches.append(patch)\n",
        "\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def split_train_into_train_val(train_file_ids, train_file_paths, train_labels, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Split train_file_paths and train_labels to train_file_paths, val_file_paths and\n",
        "    train_labels, val_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mapping between image_id and file_path\n",
        "    image_id_name_map = dict(zip(train_file_ids, train_file_paths))\n",
        "\n",
        "    # Get validation files and validation labels separate\n",
        "    train_file_ids, val_file_ids, train_labels, val_labels = train_test_split(\n",
        "        train_file_ids, train_labels, test_size=test_size, random_state=5, shuffle=True\n",
        "    )\n",
        "    train_file_paths = [image_id_name_map[image_id] for image_id in train_file_ids]\n",
        "    val_file_paths = [image_id_name_map[image_id] for image_id in val_file_ids]\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels\", len(train_labels))\n",
        "    print (\"Length of val files list\", len(val_file_paths))\n",
        "    print (\"Length of val labels\", len(val_labels))\n",
        "\n",
        "    return train_file_ids, val_file_ids, train_file_paths, val_file_paths, train_labels, val_labels\n",
        "\n",
        "def get_paths():\n",
        "    data_dir = images_dir\n",
        "    file_paths_to_return = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_paths_to_return.append(data_dir+'/'+file)\n",
        "\n",
        "    if len(file_paths_to_return) == 0:\n",
        "      raise ArgumentError(\"Data was not found. Ensure that a data folder is present\")\n",
        "\n",
        "    return file_paths_to_return\n",
        "\n",
        "def get_train_test_file_paths_n_labels():\n",
        "    \"\"\"\n",
        "    Get array train_file_paths, train_labels, test_file_paths and test_labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    #par_data_dir = 'train'\n",
        "    images_data_dir = 'train'\n",
        "    train_test_split_file = 'train_test_split.txt'\n",
        "    images_file = 'images.txt'\n",
        "    labels_file = 'image_class_labels.txt'\n",
        "\n",
        "    # Read the images_file which stores image-id and image-name mapping\n",
        "    image_file_id_df = pd.read_csv(images_file, sep=' ', header=None)\n",
        "    image_file_id_mat = image_file_id_df.values\n",
        "    image_id_name_map = dict(zip(image_file_id_mat[:, 0], image_file_id_mat[:, 1]))\n",
        "\n",
        "    # Read the train_test_split file which stores image-id and train-test split mapping\n",
        "    image_id_train_test_split_df = pd.read_csv(train_test_split_file, sep=' ', header=None)\n",
        "    image_id_train_test_split_mat = image_id_train_test_split_df.values\n",
        "    image_id_train_test_split_map = dict(zip(image_id_train_test_split_mat[:, 0],\n",
        "                                             image_id_train_test_split_mat[:, 1]))\n",
        "\n",
        "    # Read the image class labels file\n",
        "    image_id_label_df = pd.read_csv(labels_file, sep=' ', header=None)\n",
        "    image_id_label_mat = image_id_label_df.values\n",
        "    image_id_label_map = dict(zip(image_id_label_mat[:, 0], image_id_label_mat[:, 1]))\n",
        "\n",
        "    # Put together train_files train_labels test_files and test_labels lists\n",
        "    train_image_ids, test_image_ids = [], []\n",
        "    train_file_paths, test_file_paths = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "    for file_id in image_id_name_map.keys():\n",
        "        file_name = image_id_name_map[file_id]\n",
        "        is_train = image_id_train_test_split_map[file_id]\n",
        "        label = image_id_label_map[file_id] - 1  # To ensure labels start from 0\n",
        "\n",
        "        if is_train:\n",
        "            train_image_ids.append(file_id)\n",
        "            train_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            train_labels.append(label)\n",
        "        else:\n",
        "            test_image_ids.append(file_id)\n",
        "            test_file_paths.append(os.path.join(images_data_dir, file_name))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    print (\"Length of train files list\", len(train_file_paths))\n",
        "    print (\"Length of train labels list\", len(train_labels))\n",
        "    print (\"Length of test files list\", len(test_file_paths))\n",
        "    print (\"Length of test labels list\", len(test_labels))\n",
        "\n",
        "    return train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpAZGXKmiTpR"
      },
      "source": [
        "# Generate Jigsaw from permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "sm_sOs8piTpR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms.functional import rotate\n",
        "\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        'Initialization'\n",
        "        self.imgs = [(img_path, label) for img_path, label in zip(file_paths, labels)]\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            label = self.labels[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        tr_image = self.transform(pil_image)\n",
        "\n",
        "        return tr_image, label\n",
        "\n",
        "\n",
        "class GetJigsawPuzzleDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None, jig_size=4):\n",
        "        'Initialization'\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
        "        self.jig_size = jig_size\n",
        "        if range_permut_indices != None:\n",
        "          self.range_permut_indices = range_permut_indices\n",
        "        else:\n",
        "          self.range_permut_indices = (0, len(self.permuts_avail) - 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Select sample\n",
        "        file_path = self.file_paths[index]\n",
        "        pil_image = Image.open(file_path)\n",
        "\n",
        "        # Check if image has only single channel. If True, then swap with 0th image\n",
        "        # Assumption 0th image has got 3 number of channels\n",
        "        if len(pil_image.getbands()) != 3:\n",
        "            file_path = self.file_paths[0]\n",
        "            pil_image = Image.open(file_path)\n",
        "\n",
        "        # Convert image to torch tensor\n",
        "        pil_image = pil_image.resize((256, 256))\n",
        "        new_size = 256 - (256 % self.jig_size) # Ensure that the number of pixels fits the number of jigsaw pieces\n",
        "        pil_image = crop_from_center(pil_image, new_size, new_size)\n",
        "\n",
        "        # Split image into tiles (patches)\n",
        "        crops = get_several_crops(pil_image, self.jig_size)\n",
        "\n",
        "        # Generate a rotation sequence\n",
        "        rot_config = np.random.randint(4, size=self.jig_size ** 2)\n",
        "\n",
        "        # Permut the patches obtained from the image\n",
        "        if self.range_permut_indices:\n",
        "            permut_ind = int(random.randint(self.range_permut_indices[0], self.range_permut_indices[1]))\n",
        "        else:\n",
        "            permut_ind = int(random.randint(0, len(self.permuts_avail) - 1))\n",
        "\n",
        "        permutation_config = self.permuts_avail[permut_ind]\n",
        "\n",
        "        permuted_patches_arr = [None] * self.jig_size * self.jig_size\n",
        "        for crop_new_pos, rotation, crop in zip(permutation_config, rot_config, crops):\n",
        "            if rotation > 0:\n",
        "                crop = rotate(crop, angle=int(90*rotation))\n",
        "            permuted_patches_arr[crop_new_pos] = crop\n",
        "\n",
        "        # Apply data transforms\n",
        "        tensor_patches = torch.zeros(self.jig_size ** 2, 3, 64, 64) # (self.jig_size ** 2, number channels, patch height, patch width)\n",
        "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
        "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
        "            tensor_patches[ind] = jigsaw_patch_tr\n",
        "\n",
        "        return tensor_patches, permutation_config, rot_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itXKzIqhiTpT"
      },
      "source": [
        "# Defining Resnet model\n",
        "Credit: https://github.com/aniket03/self_supervised_bird_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "BEYNDi47iTpT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, jigsaw_size=3, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None, siamese_deg=9, train_contrastive=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.siamese_deg = siamese_deg\n",
        "        self.train_contrastive = train_contrastive\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "\n",
        "        self.pos_head = nn.Linear(2048 * block.expansion, jigsaw_size ** 2)\n",
        "        self.rot_head = nn.Linear(2048 * block.expansion, 4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_feature_vectors(self, input_batch):\n",
        "        # Each input_batch would be of shape (batch_size, color_channels, h, w)\n",
        "        x = self.conv1(input_batch)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        B, N, C, H, W = input_batch.shape\n",
        "        print(f\"Input batch shape: {input_batch.shape}\")\n",
        "        patch_features = []\n",
        "\n",
        "        for i in range(N):\n",
        "            feat = self.get_feature_vectors(input_batch[:, i, :, :, :])\n",
        "            patch_features.append(feat)\n",
        "        patch_features = torch.stack(patch_features, dim=1)  # [B, 9, feat_dim]\n",
        "        print(f\"Patch feat shape: {patch_features.shape}\")\n",
        "\n",
        "        # Predict position and rotation for each patch\n",
        "        pos_logits = self.pos_head(patch_features)  # [B, 9, 9]\n",
        "        print(f\"Pos logits shape: {pos_logits.shape}\")\n",
        "        rot_logits = self.rot_head(patch_features)  # [B, 9, 4]\n",
        "        print(f\"Rot logits shape: {rot_logits.shape}\")\n",
        "\n",
        "        return pos_logits, rot_logits\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    \"\"\"\n",
        "    return _resnet(BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "O02gxDG-iTpU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "def get_count_correct_preds(network_output, target):\n",
        "\n",
        "    output = network_output\n",
        "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "    pred.data = pred.data.view_as(target.data)\n",
        "    correct = target.eq(pred).sum().item()\n",
        "\n",
        "    return correct\n",
        "\n",
        "\n",
        "class ModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(ModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
        "            data, target = Variable(data).to(self.device), Variable(target).to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.network(data)\n",
        "\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(self.network.parameters(), params_max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "            train_loss += loss.item()\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        train_acc = correct / len(train_data_loader.dataset)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, train_loss, correct, len(train_data_loader.dataset),\n",
        "            100. * correct / len(train_data_loader.dataset)))\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
        "            data, target = Variable(data, volatile=True).to(self.device), Variable(target).to(self.device)\n",
        "            output = self.network(data)\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
        "\n",
        "            correct += get_count_correct_preds(output, target)\n",
        "\n",
        "            del data, target, output\n",
        "\n",
        "        test_loss /= len(test_data_loader.dataset)\n",
        "        test_acc = correct / len(test_data_loader.dataset)\n",
        "        print('\\nAfter epoch {} - Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, test_loss, correct, len(test_data_loader.dataset),\n",
        "            100. * correct / len(test_data_loader.dataset)))\n",
        "\n",
        "        return  test_loss, test_acc\n",
        "\n",
        "\n",
        "class JigsawModelTrainTest():\n",
        "\n",
        "    def __init__(self, network, device, model_file_path, threshold=1e-4):\n",
        "        super(JigsawModelTrainTest, self).__init__()\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.model_file_path = model_file_path\n",
        "        self.threshold = threshold\n",
        "        self.train_loss = 1e9\n",
        "        self.val_loss = 1e9\n",
        "\n",
        "    def train(self, optimizer, epoch, params_max_norm, train_data_loader, val_data_loader):\n",
        "        self.network.train()\n",
        "        train_loss = 0\n",
        "        cnt_batches = 0\n",
        "        pos_correct = 0\n",
        "        rot_correct = 0\n",
        "        total_pred_pos = 0\n",
        "        total_pred_rot = 0\n",
        "        batch_size = 0\n",
        "\n",
        "        for batch_idx, (data, pos_vector, rot_vector) in enumerate(train_data_loader):\n",
        "\n",
        "            data, pos_vector, rot_vector = Variable(data).to(self.device), Variable(pos_vector).to(self.device), Variable(rot_vector).to(self.device)\n",
        "            batch_size = data.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pos_log, rot_log = self.network(data)\n",
        "\n",
        "            pos_pred = pos_log.argmax(dim=2)  # Shape (B, patches)\n",
        "            rot_pred = rot_log.argmax(dim=2)\n",
        "\n",
        "            pos_log = pos_log.permute(0, 2, 1) # Shape (B, patches, patches)\n",
        "            rot_log = rot_log.permute(0, 2, 1) # Shape (B, 4, patches)\n",
        "\n",
        "\n",
        "            loss_pos = F.cross_entropy(pos_log, pos_vector) # Shape (B, patches, 9)\n",
        "            loss_rot = F.cross_entropy(rot_log, rot_vector)\n",
        "            loss = loss_pos + loss_rot\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            pos_correct += pos_pred.eq(pos_vector).sum() # Single number\n",
        "            rot_correct += rot_pred.eq(rot_vector).sum() # Single number\n",
        "            total_pred_pos += pos_pred.shape[0] * pos_pred.shape[1]\n",
        "            total_pred_rot += rot_pred.shape[0] * rot_pred.shape[1]\n",
        "            cnt_batches += 1\n",
        "\n",
        "            del data, pos_vector, rot_vector, pos_log, rot_log\n",
        "\n",
        "        train_loss /= cnt_batches\n",
        "        val_loss, val_acc = self.test(epoch, val_data_loader)\n",
        "\n",
        "        if val_loss < self.val_loss - self.threshold:\n",
        "            self.val_loss = val_loss\n",
        "            torch.save(self.network.state_dict(), self.model_file_path)\n",
        "\n",
        "        correct = (pos_correct + rot_correct) / 2\n",
        "        total_pred = total_pred_pos * cnt_batches\n",
        "\n",
        "        train_acc = correct / (total_pred_pos + total_pred_rot)\n",
        "\n",
        "        print('\\nAfter epoch {} - Train set: Number of batches: {}, Batch size: {}, Average loss: {:.4f}, Total accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, cnt_batches, batch_size, train_loss, correct, total_pred,\n",
        "            100. * correct / total_pred))\n",
        "        print(f'Position accuracy: {pos_correct}/{total_pred}, Rotation accuracy: {rot_correct}/{total_pred}')\n",
        "\n",
        "\n",
        "\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def test(self, epoch, test_data_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        pos_correct = 0\n",
        "        rot_correct = 0\n",
        "        total_pred_pos = 0\n",
        "        total_pred_rot = 0\n",
        "        batch_size = 0\n",
        "        cnt_batches = 0\n",
        "\n",
        "        for batch_idx, (data, pos_vector, rot_vector) in enumerate(test_data_loader):\n",
        "            data, pos_vector, rot_vector = Variable(data).to(self.device), Variable(pos_vector).to(self.device), Variable(rot_vector).to(self.device)\n",
        "            pos_log, rot_log = self.network(data)\n",
        "            batch_size = data.shape[0]\n",
        "\n",
        "            pos_log, rot_log = self.network(data)\n",
        "            pos_pred = pos_log.argmax(dim=2)  # Shape (B, patches)\n",
        "            rot_pred = rot_log.argmax(dim=2)\n",
        "            pos_log = pos_log.permute(0, 2, 1) # Shape (B, patches, patches)\n",
        "            rot_log = rot_log.permute(0, 2, 1) # Shape (B, 4, patches)\n",
        "            loss_pos = F.cross_entropy(pos_log, pos_vector).item() # Shape (B, patches, 9)\n",
        "            loss_rot = F.cross_entropy(rot_log, rot_vector).item()\n",
        "            test_loss += (loss_pos + loss_rot) / 2\n",
        "\n",
        "\n",
        "            pos_correct += pos_pred.eq(pos_vector).sum() # Single number\n",
        "            rot_correct += rot_pred.eq(rot_vector).sum() # Single number\n",
        "\n",
        "            # print(f\"Pos_pred: {pos_pred}\")\n",
        "            # print(f\"Rot_pred: {rot_pred}\")\n",
        "            # print(f\"pos_pred.eq(pos_vector): {pos_pred.eq(pos_vector)}\")\n",
        "            # print(f\"rot_pred.eq(rot_vector): {rot_pred.eq(rot_vector)}\")\n",
        "            # print(f\"pos_pred.eq(pos_vector).sum(): {pos_pred.eq(pos_vector).sum()}\")\n",
        "            # print(f\"rot_pred.eq(rot_vector).sum(): {rot_pred.eq(rot_vector).sum()}\")\n",
        "\n",
        "            total_pred_pos += pos_pred.shape[0] * pos_pred.shape[1]\n",
        "            total_pred_rot += rot_pred.shape[0] * rot_pred.shape[1]\n",
        "            cnt_batches += 1\n",
        "\n",
        "\n",
        "            del data, pos_vector, rot_vector, pos_log, rot_log\n",
        "\n",
        "        correct = (pos_correct + rot_correct) / 2\n",
        "        total_pred = total_pred_pos * cnt_batches\n",
        "\n",
        "        test_loss /= cnt_batches\n",
        "        test_acc = correct / total_pred\n",
        "        print('\\nAfter epoch {} - Test set: Number of batches: {}, Batch size: {}, Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch, len(test_data_loader), batch_size, test_loss, correct, total_pred,\n",
        "            100. * correct / total_pred))\n",
        "\n",
        "        return  test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv25ckxKiTpV"
      },
      "source": [
        "# Jigsaw as pretext task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qB6edlUIiTpV",
        "outputId": "3b8ea40c-d5f1-4c58-dcb4-7c09b254d6b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders done\n",
            "torch.Size([32, 16, 3, 64, 64])\n",
            "torch.Size([32, 16])\n",
            "Model ready\n",
            "Started training\n",
            "Epoch no 0 #######################\n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0005\n",
            ")\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7e7eb480a0c0>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7e7ec3a35c70>\n",
            "Input batch shape: torch.Size([32, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([32, 16, 2048])\n",
            "Pos logits shape: torch.Size([32, 16, 16])\n",
            "Rot logits shape: torch.Size([32, 16, 4])\n",
            "Input batch shape: torch.Size([2, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([2, 16, 2048])\n",
            "Pos logits shape: torch.Size([2, 16, 16])\n",
            "Rot logits shape: torch.Size([2, 16, 4])\n",
            "Input batch shape: torch.Size([17, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([17, 16, 2048])\n",
            "Pos logits shape: torch.Size([17, 16, 16])\n",
            "Rot logits shape: torch.Size([17, 16, 4])\n",
            "Input batch shape: torch.Size([17, 16, 3, 64, 64])\n",
            "Patch feat shape: torch.Size([17, 16, 2048])\n",
            "Pos logits shape: torch.Size([17, 16, 16])\n",
            "Rot logits shape: torch.Size([17, 16, 4])\n",
            "\n",
            "After epoch 0 - Test set: Number of batches: 1, Batch size: 17, Average loss: 4.3487, Accuracy: 48.5/272 (18%)\n",
            "\n",
            "\n",
            "After epoch 0 - Train set: Number of batches: 2, Batch size: 2, Average loss: 4.9624, Total accuracy: 77.0/1088 (7%)\n",
            "\n",
            "Position accuracy: 29/1088, Rotation accuracy: 125/1088\n",
            "Train loss 4.962377071380615 \n",
            " Val loss 4.348728656768799 \n",
            " Train Acc 0.07077205926179886 \n",
            " Val Acc 0.17830882966518402\n",
            "val_losses: [4.348728656768799]\n",
            "type val loss: <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "#for jigsaw ssl task\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Cexperiment_name = 'e1_js'\n",
        "    Cdataset_config = 'js_d1'\n",
        "    Cweight_decay = 5e-4\n",
        "    Clr = 1e-2\n",
        "    Cepochs = 1\n",
        "    Cbatch_size = 32\n",
        "\n",
        "    # Data files which will get referred\n",
        "    permuts_file_path = 'selected_permuts.npy'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = 'resnet_jigsaw_solver_{}_trained.pt'.format(Cexperiment_name)\n",
        "\n",
        "    all_file_paths = get_paths()\n",
        "\n",
        "    # Get validation files separate\n",
        "    train_file_paths, val_file_paths = train_test_split(all_file_paths, test_size=0.1, shuffle=True, random_state=3)\n",
        "\n",
        "    # Compute channel means\n",
        "    channel_means = np.array([124.09, 127.67, 110.50]) / 256.0\n",
        "\n",
        "    # Define data transforms for each individual tile\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.RandomCrop((64, 64)),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Define data loaders\n",
        "    batch_size = Cbatch_size\n",
        "    jig_size = 4\n",
        "\n",
        "    if Cdataset_config == 'js_d1':\n",
        "        n = jig_size**2 + 1\n",
        "        train_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [GetJigsawPuzzleDataset(train_file_paths, permuts_file_path,\n",
        "                                      transform=data_transform, jig_size=jig_size)\n",
        "                 for st_perm_ind in range(0, n**2, n)\n",
        "                ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            ConcatDataset(\n",
        "                [GetJigsawPuzzleDataset(val_file_paths, permuts_file_path,\n",
        "                                        transform=data_transform, jig_size=jig_size)\n",
        "                 for st_perm_ind in range(0, n**2, n)\n",
        "                 ]\n",
        "            ),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "    else:\n",
        "        train_data_loader = DataLoader(\n",
        "            GetJigsawPuzzleDataset(train_file_paths, permuts_file_path, transform=data_transform, jig_size=jig_size),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "        val_data_loader = DataLoader(\n",
        "            GetJigsawPuzzleDataset(val_file_paths, permuts_file_path, transform=data_transform, jig_size=jig_size),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "    print(\"Loaders done\")\n",
        "    # Print sample batches that would be returned by the train_data_loader\n",
        "    dataiter = iter(train_data_loader)\n",
        "    X, y, r = dataiter.__next__() # Returns patches, positions, rotations\n",
        "    print (X.size())\n",
        "    print (y.size())\n",
        "\n",
        "    # Train required model defined above on CUB200 data\n",
        "    num_outputs = num_permuts_saved#200\n",
        "    epochs = Cepochs\n",
        "    lr = Clr\n",
        "    weight_decay_const = Cweight_decay\n",
        "\n",
        "    # If using Resnet18\n",
        "    model_to_train = resnet18(num_classes=num_outputs, siamese_deg=jig_size**2, jigsaw_size=jigsaw_puzzle_size)\n",
        "    print('Model ready')\n",
        "    # Set device on which training is done. Plus optimizer to use.\n",
        "    model_to_train.to(device)\n",
        "    optimizer = optim.SGD(model_to_train.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay_const)\n",
        "    # scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True, min_lr=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, min_lr=1e-5)\n",
        "\n",
        "    # Start training\n",
        "    print('Started training')\n",
        "    model_train_test_obj = JigsawModelTrainTest(model_to_train, device, model_file_path)\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    for epoch_no in range(epochs):\n",
        "        print(\"Epoch no {} #######################\".format(epoch_no))\n",
        "        print(optimizer)\n",
        "        print(train_data_loader)\n",
        "        print(val_data_loader)\n",
        "        train_loss, train_acc, val_loss, val_acc = model_train_test_obj.train(\n",
        "            optimizer, epoch_no, params_max_norm=4,\n",
        "            train_data_loader = train_data_loader, val_data_loader = val_data_loader\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"Train loss {} \\n Val loss {} \\n Train Acc {} \\n Val Acc {}\".format(train_loss,val_loss,train_acc,val_acc))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    observations_df = pd.DataFrame()\n",
        "    observations_df['epoch count'] = [i for i in range(1, Cepochs + 1)]\n",
        "    observations_df['train loss'] = train_losses\n",
        "    print(f\"val_losses: {val_losses}\")\n",
        "    print(f\"type val loss: {type(val_losses)}\")\n",
        "    observations_df['val loss'] = val_losses\n",
        "    observations_df['train acc'] = train_accs\n",
        "    observations_df['val acc'] = val_accs\n",
        "    observations_file_path = Cexperiment_name + '_observations.csv'\n",
        "    observations_df.to_csv(observations_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_file_paths)"
      ],
      "metadata": {
        "id": "WrbRVKMY6qmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129bcb4a-b0ce-48dc-b915-5a697b4cd3b6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['extraimages/extra-image-1.jpg', 'extraimages/extra-image-15.jpg', 'extraimages/extra-image-3.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lbE1N98iTpV"
      },
      "source": [
        "# Plot loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "XAuZaAkCiTpV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "efe5d12a-77ae-4c47-960d-d98757643e9c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('Loss plots')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "R5VXGhp7iTpV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "a2e07ab9-390a-41c1-b5d3-c5f25a97c1c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs)\n",
        "plt.plot(val_accs)\n",
        "plt.title('Accuracy plots')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "num_outputs = num_permuts_saved\n",
        "siamese_deg = jigsaw_puzzle_size ** 2\n",
        "\n",
        "# Use same transforms as training\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomCrop((64, 64)),\n",
        "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "data_loader = GetDataset([\"mountain_Solution.jpg\"], [None], transform=data_transform)\n",
        "jig_loader = GetJigsawPuzzleDataset([\"mountain_Solution.jpg\"], 'selected_permuts.npy', transform=data_transform, jig_size=jigsaw_puzzle_size)\n",
        "\n",
        "\n",
        "model = resnet18(num_classes=num_outputs, siamese_deg=siamese_deg, jigsaw_size=jigsaw_puzzle_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"resnet_jigsaw_solver_e1_js_trained.pt\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "X, y = jig_loader[0]\n",
        "X = X.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model(X)\n",
        "\n",
        "print(\"Output shape: \", output.shape)\n",
        "out_im = transforms.functional.to_pil_image(output)\n",
        "\n",
        "out_im.show()\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "t7bPiDL4x2ZB",
        "outputId": "cda4689f-ed2a-4c5b-e447-84c53092177f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mountain_Solution.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4069390006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjig_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1835160576.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Select sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mpil_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Check if image has only single channel. If True, then swap with 0th image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mountain_Solution.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqvSlGHSiTpW"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    pil_img = Image.open(path)\n",
        "    if pil_img.mode == \"L\":\n",
        "        return None\n",
        "    else:\n",
        "        return pil_img\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    Cmodel_name = 'resnet_trained_ssl_e1_last_b_last_b_ft.pt'\n",
        "    Ctest_compact_bilinear = True\n",
        "    Ctest_imagenet_based = False\n",
        "    Ctest_on = 'test'\n",
        "\n",
        "    # Set device to use to gpu if available and declare model_file_path\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #par_weights_dir = 'weights/'\n",
        "    model_file_path = Cmodel_name\n",
        "\n",
        "    # Data loading and data generators set up\n",
        "    train_image_ids, test_image_ids, train_file_paths, test_file_paths, train_labels, test_labels = \\\n",
        "        get_train_test_file_paths_n_labels()\n",
        "\n",
        "    train_image_ids, val_image_ids, train_file_paths, val_file_paths, train_labels, val_labels = \\\n",
        "        split_train_into_train_val(train_image_ids, train_file_paths, train_labels, test_size=0.1)\n",
        "\n",
        "    if Ctest_imagenet_based:\n",
        "        model_to_train = models.resnet18(pretrained=True)\n",
        "        model_to_train.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
        "        model_to_train.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048, 5),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "    else:\n",
        "        model_to_train = resnet18(num_classes=5, siamese_deg=None, jigsaw_size=jigsaw_puzzle_size)\n",
        "\n",
        "    # Check if saved model exists, and load if it does.\n",
        "    if os.path.exists(model_file_path):\n",
        "        model_to_train.load_state_dict(torch.load(model_file_path))\n",
        "    model_to_train.to(device)\n",
        "\n",
        "    # Setup on which set evaluation is to be carried out\n",
        "    if Ctest_on == 'train':\n",
        "        eval_file_paths, eval_labels = train_file_paths, train_labels\n",
        "    elif Ctest_on == 'val':\n",
        "        eval_file_paths, eval_labels = val_file_paths, val_labels\n",
        "    else:\n",
        "        eval_file_paths, eval_labels = test_file_paths, test_labels\n",
        "\n",
        "    # Start evaluation\n",
        "    model_to_train.eval()\n",
        "    correct = 0\n",
        "    preds = []\n",
        "    for f, label in zip(eval_file_paths, eval_labels):\n",
        "        pil_img = pil_loader(f)\n",
        "        if pil_img is None:\n",
        "            preds.append(0)\n",
        "            continue\n",
        "        data = def_data_transform(pil_img)\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        data = Variable(data, volatile=True).to(device)\n",
        "        output = model_to_train(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "\n",
        "        x = pred.data #prediction\n",
        "        preds.append(x)\n",
        "\n",
        "        if x == label:\n",
        "            correct += 1\n",
        "\n",
        "    print (correct, len(eval_file_paths), correct * 100 / len(eval_file_paths))\n",
        "    preds = np.array(preds).astype(np.float64)\n",
        "    conf_mat = np.array(confusion_matrix(preds, eval_labels))\n",
        "    conf_df = pd.DataFrame(conf_mat)\n",
        "    conf_df.columns = np.arange(0,5)\n",
        "    conf_df.to_csv('confusion_matrix.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}